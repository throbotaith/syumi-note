{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ロバスト動的計画法の実装\n",
    "* pythonで書いていきます〜\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ロバストVIアルゴリズム\n",
    "\n",
    "**入力:**  \n",
    "$V \\in \\mathcal{V}, \\epsilon > 0$\n",
    "\n",
    "**出力:**  \n",
    "$\\tilde{V}$ ただし、$\\|\\tilde{V} - V^{*}\\| \\leq \\frac{\\epsilon}{2}$\n",
    "\n",
    "- 各 $s \\in \\mathcal{S}$ に対して、$\\tilde{V}(s)$ を次のように設定する：\n",
    "  $$\n",
    "  \\tilde{V}(s) = \\sup_{a \\in \\mathcal{A}(s)} \\left\\{ \\inf_{p \\in \\mathcal{P}(s, a)} \\mathbf{E}^{p} \\left[ r(r, a, s') + \\lambda V(s') \\right] \\right\\}\n",
    "  $$\n",
    "\n",
    "- while $\\left( \\|\\tilde{V} - V\\| \\geq \\frac{(1 - \\lambda)}{\\lambda} \\cdot \\epsilon \\right)$ do:\n",
    "\n",
    "  - $V = \\tilde{V}$\n",
    "\n",
    "  - $\\forall s \\in \\mathcal{S}$, 次のように $\\tilde{V}(s)$ を設定する：\n",
    "    $$\n",
    "    \\tilde{V}(s) = \\sup_{a \\in \\mathcal{A}(s)} \\left\\{ \\inf_{p \\in \\mathcal{P}(s, a)} \\mathbf{E}^{p} \\left[ r(r, a, s') + \\lambda V(s') \\right] \\right\\}\n",
    "    $$\n",
    "\n",
    "- end while\n",
    "\n",
    "**戻り値:**  \n",
    "$\\tilde{V}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VI: [-0.0468559 -0.0468559 -0.0468559]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def robust_value_iteration(initial_value_function, tolerance, states, actions, transition_probabilities, reward_function, discount_factor):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    initial_value_function: 初期価値関数 \n",
    "    tolerance: 許容誤差 \n",
    "    states: 状態空間 \n",
    "    actions: 行動空間 \n",
    "    transition_probabilities: 状態遷移確率の集合 \n",
    "    reward_function: 報酬関数 \n",
    "    discount_factor: 割引率\n",
    "  Returns:\n",
    "    V\n",
    "  \"\"\"\n",
    "\n",
    "  value_function = initial_value_function.copy()\n",
    "  updated_value_function = np.zeros_like(value_function)\n",
    "\n",
    "  while True:\n",
    "    for state in states:\n",
    "      state_values = []\n",
    "      for action in actions:\n",
    "        # 各行動に対する最悪ケースの期待報酬を計算\n",
    "        worst_case_expected_reward = min([\n",
    "            np.dot(transition_probability, reward_function(state, action, states) + discount_factor * value_function)\n",
    "            for transition_probability in transition_probabilities[(state, action)]\n",
    "        ])\n",
    "        state_values.append(worst_case_expected_reward)\n",
    "      # 最適な行動を選択\n",
    "      updated_value_function[state] = max(state_values)\n",
    "\n",
    "    # 価値関数の更新量が許容誤差以下になったら終了\n",
    "    if np.linalg.norm(updated_value_function - value_function) < tolerance:\n",
    "      break\n",
    "\n",
    "    # 価値関数を更新\n",
    "    value_function = updated_value_function.copy()\n",
    "\n",
    "  return value_function\n",
    "\n",
    "# 状態空間と行動空間\n",
    "states = [0, 1, 2]\n",
    "actions = [0, 1]\n",
    "\n",
    "# 遷移確率の集合 (例)\n",
    "transition_probabilities = {\n",
    "    (0, 0): [np.array([0.7, 0.3, 0.0]), np.array([0.8, 0.2, 0.0])],\n",
    "    (0, 1): [np.array([0.2, 0.6, 0.2]), np.array([0.3, 0.5, 0.2])],\n",
    "    (1, 0): [np.array([0.1, 0.8, 0.1]), np.array([0.0, 0.9, 0.1])],\n",
    "    (1, 1): [np.array([0.0, 0.3, 0.7]), np.array([0.0, 0.2, 0.8])],\n",
    "    (2, 0): [np.array([0.6, 0.0, 0.4]), np.array([0.5, 0.1, 0.4])],\n",
    "    (2, 1): [np.array([0.3, 0.2, 0.5]), np.array([0.4, 0.1, 0.5])]\n",
    "}\n",
    "\n",
    "# 報酬関数\n",
    "def reward_function(state, action, next_state):\n",
    "  return 1.0 if next_state == 2 else -0.01\n",
    "\n",
    "# 割引率\n",
    "discount_factor = 0.9\n",
    "\n",
    "# 初期価値関数と許容誤差\n",
    "initial_value_function = np.zeros(len(states))\n",
    "tolerance = 0.01\n",
    "\n",
    "# ロバストな価値反復の実行\n",
    "optimal_value_function = robust_value_iteration(\n",
    "    initial_value_function, tolerance, states, actions, transition_probabilities, reward_function, discount_factor\n",
    ")\n",
    "\n",
    "print(\"VI:\", optimal_value_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ロバスト方策反復アルゴリズム\n",
    "\n",
    "**入力:**  \n",
    "決定規則 $d_{0}$、$\\epsilon > 0$\n",
    "\n",
    "**出力:**  \n",
    "$\\epsilon$-最適な決定規則 $d^{*}$\n",
    "\n",
    "1. $n = 0$ および $\\pi_{n} = (d_{n}, d_{n}, \\ldots)$ を設定する。式 (31) を解いて $V^{\\pi_{n}}$ を計算する。$\\tilde{V} \\leftarrow L_{\\mathcal{D}} V^{\\pi_{n}}$ とし、$\\mathcal{D} = \\prod_{s \\in \\mathcal{S}} \\mathcal{A}(s)$ を設定する。\n",
    "\n",
    "2. 各 $s \\in \\mathcal{S}$ に対して、次の条件を満たす $d_{n+1}(s)$ を選択する：\n",
    "   $$\n",
    "   d_{n+1}(s) \\in \\left\\{ a \\in \\mathcal{A}(s) : \\inf_{p \\in \\mathcal{P}(s, a)} \\mathbf{E}^{p} \\left[ r(s, a, s') + \\lambda V(s') \\right] \\geq \\tilde{V}(s) - \\epsilon \\right\\}\n",
    "   $$\n",
    "   可能であれば、$d_{n+1}(s) = d_{n}(s)$ を設定する。\n",
    "\n",
    "3. while $d_{n+1} \\neq d_{n}$ do:\n",
    "   - $n = n + 1$ とする。\n",
    "   - 式 (31) を解いて $V^{\\pi_{n}}$ を計算する。$\\tilde{V} \\leftarrow L_{\\mathcal{D}} V^{\\pi_{n}}$ とし、$\\mathcal{D} = \\prod_{s \\in \\mathcal{S}} \\mathcal{A}(s)$ を設定する。\n",
    "   - 各 $s \\in \\mathcal{S}$ に対して、次の条件を満たす $d_{n+1}(s)$ を選択する：\n",
    "     $$\n",
    "     d_{n+1}(s) \\in \\left\\{ a \\in \\mathcal{A}(s) : \\inf_{p \\in \\mathcal{P}(s, a)} \\mathbf{E}^{p} \\left[ r(s, a, s') + \\lambda V(s') \\right] \\geq \\tilde{V}(s) - \\epsilon \\right\\}\n",
    "     $$\n",
    "     可能であれば、$d_{n+1}(s) = d_{n}(s)$ を設定する。\n",
    "\n",
    "4. end while\n",
    "\n",
    "**戻り値:**  \n",
    "$d_{n+1}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ε-最適な決定規則: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def robust_policy_iteration(initial_decision_rule, tolerance, states, actions, transition_probabilities, reward_function, discount_factor):\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  Args:\n",
    "    initial_decision_rule: 初期決定規則 (function: (状態) -> 行動)\n",
    "        各状態に対して行動を決定する関数です。\n",
    "    tolerance: 許容誤差 (float)\n",
    "        決定規則の更新量がこの値以下になったらアルゴリズムを終了します。\n",
    "    states: 状態空間 (list)\n",
    "        問題におけるすべての状態のリストです。\n",
    "    actions: 行動空間 (list)\n",
    "        問題におけるすべての行動のリストです。\n",
    "    transition_probabilities: 状態遷移確率の集合 (dictionary: (状態, 行動) -> list of numpy arrays)\n",
    "        各状態-行動ペア (s, a) に対して、可能な遷移確率を表すnumpy配列のリストを値として持ちます。\n",
    "        各numpy配列は、状態sで行動aを取ったときに、各次の状態に遷移する確率を表します。\n",
    "    reward_function: 報酬関数 (function: (状態, 行動, 次の状態) -> float)\n",
    "        現在の状態、行動、次の状態を引数に取り、その遷移で得られる報酬を返す関数です。\n",
    "    discount_factor: 割引率 (float, 0 < discount_factor < 1)\n",
    "        将来の報酬を現在価値に割り引くための係数です。\n",
    "\n",
    "  Returns:\n",
    "    ε-最適な決定規則 (function: (状態) -> 行動)\n",
    "  \"\"\"\n",
    "\n",
    "  # 決定規則を初期化\n",
    "  decision_rule = initial_decision_rule\n",
    "\n",
    "  # 反復回数\n",
    "  iteration_count = 0\n",
    "\n",
    "  while True:\n",
    "    # 反復回数をインクリメント\n",
    "    iteration_count += 1\n",
    "\n",
    "    # 現在の決定規則に基づいて、定常方策πnを作成\n",
    "    def policy(state):\n",
    "      return decision_rule(state)\n",
    "\n",
    "    # 方策評価: 式(31)を解いて、現在のポリシーの価値関数を計算\n",
    "    value_function = evaluate_policy(policy, tolerance, states, actions, transition_probabilities, reward_function, discount_factor)\n",
    "\n",
    "    # 方策改善: 各状態において、ε-greedyに新たな決定規則を計算\n",
    "    updated_decision_rule = improve_policy(value_function, tolerance, states, actions, transition_probabilities, reward_function, discount_factor)\n",
    "\n",
    "    # 決定規則が更新されなくなったら終了\n",
    "    if is_decision_rule_converged(updated_decision_rule, decision_rule, states):\n",
    "      break\n",
    "\n",
    "    # 決定規則を更新\n",
    "    decision_rule = updated_decision_rule\n",
    "\n",
    "    # デバッグ: 反復回数と決定規則を出力\n",
    "    print(f\"Iteration: {iteration_count}, Decision Rule: {decision_rule}\")\n",
    "\n",
    "  # 収束した決定規則を返す\n",
    "  return decision_rule\n",
    "\n",
    "\n",
    "def evaluate_policy(policy, tolerance, states, actions, transition_probabilities, reward_function, discount_factor):\n",
    "  \"\"\"\n",
    "\n",
    "  Args:\n",
    "    policy: 方策 (function: (状態) -> 行動)\n",
    "    tolerance: 許容誤差 (float)\n",
    "    states: 状態空間 (list)\n",
    "    actions: 行動空間 (list)\n",
    "    transition_probabilities: 状態遷移確率の集合 (dictionary: (状態, 行動) -> list of numpy arrays)\n",
    "    reward_function: 報酬関数 (function: (状態, 行動, 次の状態) -> float)\n",
    "    discount_factor: 割引率 (float)\n",
    "\n",
    "  Returns:\n",
    "    価値関数 (numpy array, shape=(状態数,))\n",
    "  \"\"\"\n",
    "\n",
    "  value_function = np.zeros(len(states))\n",
    "  while True:\n",
    "    updated_value_function = np.zeros_like(value_function)\n",
    "    for state in states:\n",
    "      action = policy(state)\n",
    "      # 期待報酬を計算 (最悪ケースの遷移確率を選択)\n",
    "      expected_reward = min([\n",
    "        np.dot(transition_probability, reward_function(state, action, states) + discount_factor * value_function)\n",
    "        for transition_probability in transition_probabilities[(state, action)]\n",
    "      ])\n",
    "      updated_value_function[state] = expected_reward\n",
    "    if np.linalg.norm(updated_value_function - value_function) < tolerance:\n",
    "      break\n",
    "    value_function = updated_value_function.copy()\n",
    "  return value_function\n",
    "\n",
    "\n",
    "def improve_policy(value_function, tolerance, states, actions, transition_probabilities, reward_function, discount_factor):\n",
    "  \"\"\"\n",
    "\n",
    "  Args:\n",
    "    value_function: 価値関数 (numpy array, shape=(状態数,))\n",
    "    tolerance: 許容誤差 (float)\n",
    "    states: 状態空間 (list)\n",
    "    actions: 行動空間 (list)\n",
    "    transition_probabilities: 状態遷移確率の集合 (dictionary: (状態, 行動) -> list of numpy arrays)\n",
    "    reward_function: 報酬関数 (function: (状態, 行動, 次の状態) -> float)\n",
    "    discount_factor: 割引率 (float)\n",
    "\n",
    "  Returns:\n",
    "    更新された決定規則 (function: (状態) -> 行動)\n",
    "  \"\"\"\n",
    "\n",
    "  def updated_decision_rule(state):\n",
    "    best_action = None\n",
    "    best_value = float('-inf')\n",
    "    for action in actions:\n",
    "      # 各行動に対する最悪ケースの期待報酬を計算\n",
    "      worst_case_expected_reward = min([\n",
    "        np.dot(transition_probability, reward_function(state, action, states) + discount_factor * value_function)\n",
    "        for transition_probability in transition_probabilities[(state, action)]\n",
    "      ])\n",
    "      if worst_case_expected_reward > best_value + tolerance:\n",
    "        best_action = action\n",
    "        best_value = worst_case_expected_reward\n",
    "    return best_action\n",
    "  return updated_decision_rule\n",
    "\n",
    "\n",
    "def is_decision_rule_converged(decision_rule1, decision_rule2, states):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    decision_rule1: 決定規則1 (function: (状態) -> 行動)\n",
    "    decision_rule2: 決定規則2 (function: (状態) -> 行動)\n",
    "    states: 状態空間 (list)\n",
    "\n",
    "  Returns:\n",
    "    収束している場合はTrue、そうでない場合はFalse\n",
    "  \"\"\"\n",
    "  for state in states:\n",
    "    if decision_rule1(state) != decision_rule2(state):\n",
    "      return False\n",
    "  return True\n",
    "\n",
    "\n",
    "  # 状態空間と行動空間\n",
    "states = [0, 1, 2]\n",
    "actions = [0, 1]\n",
    "\n",
    "# 遷移確率の集合 (例)\n",
    "transition_probabilities = {\n",
    "    (0, 0): [np.array([0.7, 0.3, 0.0]), np.array([0.8, 0.2, 0.0])],\n",
    "    (0, 1): [np.array([0.2, 0.6, 0.2]), np.array([0.3, 0.5, 0.2])],\n",
    "    (1, 0): [np.array([0.1, 0.8, 0.1]), np.array([0.0, 0.9, 0.1])],\n",
    "    (1, 1): [np.array([0.0, 0.3, 0.7]), np.array([0.0, 0.2, 0.8])],\n",
    "    (2, 0): [np.array([0.6, 0.0, 0.4]), np.array([0.5, 0.1, 0.4])],\n",
    "    (2, 1): [np.array([0.3, 0.2, 0.5]), np.array([0.4, 0.1, 0.5])]\n",
    "}\n",
    "\n",
    "# 報酬関数 \n",
    "def reward_function(state, action, next_state):\n",
    "  return 1.0 if next_state == 2 else 0.0\n",
    "\n",
    "# 割引率\n",
    "discount_factor = 0.9\n",
    "\n",
    "# 初期決定規則と許容誤差\n",
    "def initial_decision_rule(state):\n",
    "  return 0  # すべての状態に対して行動0を選択\n",
    "\n",
    "tolerance = 0.01\n",
    "\n",
    "# ロバストな方策反復の実行\n",
    "optimal_decision_rule = robust_policy_iteration(\n",
    "    initial_decision_rule, tolerance, states, actions, transition_probabilities, reward_function, discount_factor\n",
    ")\n",
    "\n",
    "print(\"ε-最適な決定規則:\", optimal_decision_rule(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
