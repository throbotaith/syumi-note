{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d26bc9e5",
   "metadata": {},
   "source": [
    "## Q学習の最適値への収束性\n",
    "Q学習の最適価値関数への収束性にかんするまとめを書きます．表記は[強化学習の青本](https://www.amazon.co.jp/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%83%95%E3%82%A7%E3%83%83%E3%82%B7%E3%83%A7%E3%83%8A%E3%83%AB%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-%E6%A3%AE%E6%9D%91-%E5%93%B2%E9%83%8E/dp/4065155916)に準拠します．\n",
    "\n",
    "### 準備\n",
    "* 時刻: $t$\n",
    "* 状態: $s_t$ \n",
    "* 行動: $a_t$ \n",
    "* 報酬: $r_t$\n",
    "* 次状態: $s_{t+1}$\n",
    "* 学習率:$\\alpha_t$\n",
    "* TD誤差:$\\delta_t^{(\\mathrm{q})}$\n",
    "\n",
    "$$\n",
    "\\delta_t^{(\\mathrm{q})} = r_t + \\gamma \\max_{a' \\in \\mathcal{A}} \\hat{Q}(s_{t+1}, a') - \\hat{Q}(s_t, a_t)\n",
    "$$\n",
    "* Q学習更新則:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\hat{Q}(s_t, a_t) := \\hat{Q}(s_t, a_t) + \\alpha_t \\delta_t^{(\\mathrm{q})} \\\\\n",
    "&= (1 - \\alpha_t) \\hat{Q}(s_t, a_t) + \\alpha_t \\left( r_t + \\gamma \\max_{a' \\in \\mathcal{A}} \\hat{Q}(s_{t+1}, a') \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Ｑ学習の収束性\n",
    "Q学習は**確率的近似**を用いた手法の一つです．Q学習は確率的近似の収束性を用いて証明します．最初に，確率的近似の一般形式を示します。\n",
    "* $v_t: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$，$v_t(s, a) = q_t(s, a)$\n",
    "* $\\mathrm{B}_t$:作用素\n",
    "* $\\tilde{\\alpha}_t(s, a) = \\alpha_t \\mathbb{I}_{\\{s=s_t\\}} \\mathbb{I}_{\\{a=a_t\\}}$\n",
    "* $X_{t}(s),Y_{t}(s)$:ノイズ\n",
    "\n",
    "**確率的近似の一般形式**\n",
    "$$\n",
    "v_{t+1}(s) := (1 - \\alpha_t(s)) v_t(s) + \\alpha_t(s) \\{ \\mathrm{B}_t v_t(s) + X_t(s) + Y_t(s) \\}\n",
    "$$\n",
    "さらに，Q学習に適用するために、以下の変形を行います。\n",
    "* 行動価値のベルマン最適作用素 $\\Upsilon_*$:$\\Upsilon_* q(s, a) \\triangleq \\mathbb{E}\\left\\{ g(s, a) + \\gamma \\max_{a' \\in \\mathcal{A}} q(S_{t+1}, a') \\mid S_t=s, A_t=a \\right\\}$\n",
    "*    $X_t(s, a)$ :ノイズ項\n",
    "$$\n",
    "X_t(s, a) \\triangleq \\begin{cases} r_t + \\gamma \\max_{a' \\in \\mathcal{A}} q_t(s_{t+1}, a') - \\Upsilon_* q_t(s, a) & (s=s_t, a=a_t) \\\\ 0 & (\\text{それ以外}) \\end{cases}\n",
    "$$\n",
    "**Q学習の確率的近似**\n",
    "$$\n",
    "q_{t+1}(s, a) := (1 - \\tilde{\\alpha}_t(s, a)) q_t(s, a) + \\tilde{\\alpha}_t(s, a) \\left( \\Upsilon_* q_t(s, a) + X_t(s, a) \\right)\n",
    "$$\n",
    "\n",
    "### Q学習の収束性:証明\n",
    "確率的近似の収束性から証明を行います．いくつかの条件を満たせばそれを証明できます．\n",
    "\n",
    "#### ロビンス・モンローの条件\n",
    "$$\n",
    "\\sum_{t=0}^{\\infty} \\tilde{\\alpha}_t(s, a) = \\sum_{t=0}^{\\infty} \\alpha_t \\mathbb{I}_{\\{s=s_t\\}} \\mathbb{I}_{\\{a=a_t\\}} = \\infty \\quad (\\text{w.p. } 1)\n",
    "$$\n",
    "$$\n",
    "\\sum_{t=0}^{\\infty} \\tilde{\\alpha}_t^2(s, a) = \\sum_{t=0}^{\\infty} \\alpha_t^2 \\mathbb{I}_{\\{s=s_t\\}} \\mathbb{I}_{\\{a=a_t\\}} < \\infty \\quad (\\text{w.p. } 1)\n",
    "$$\n",
    "* TODO：これが何を意味しているか，本質的な説明がしたい．\n",
    "\n",
    "#### $\\Upsilon_*$の条件\n",
    "$\\Upsilon_*$ は割引率 $\\gamma \\in [0, 1)$ の縮小写像であり、最適行動価値関数 $Q^*$ を唯一の不動点として持ちます。\n",
    "$$\n",
    "\\max_{s, a} |\\Upsilon_* q(s, a) - \\Upsilon_* q'(s, a)| \\leq \\gamma \\max_{s, a} |q(s, a) - q'(s, a)|\n",
    "$$\n",
    "$$\n",
    "Q^*(s, a) = \\Upsilon_* Q^*(s, a)\n",
    "$$\n",
    "\n",
    "#### $X_t$ の条件\n",
    "* $\\mathbb{E}[X_t(s, a) | \\xi_t]$ が$0$であることを示します。 $\\xi_t$ は $t$ までの履歴を表し、$q_t$ は $\\xi_t$ に対して決定論的です。$X_t(s, a)$ が非ゼロになるのは $(s, a) = (s_t, a_t)$ の場合のみなので、この場合を考えます。\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}[X_t(s_t, a_t) | \\xi_t] &= \\mathbb{E}[r_t + \\gamma \\max_{a' \\in \\mathcal{A}} q_t(s_{t+1}, a') - \\Upsilon_* q_t(s_t, a_t) | \\xi_t] \\\\\n",
    "&= \\mathbb{E}[R_t + \\gamma \\max_{a' \\in \\mathcal{A}} q_t(S_{t+1}, a') | S_t=s_t, A_t=a_t] - \\Upsilon_* q_t(s_t, a_t) \\\\\n",
    "&= \\mathbb{E}[g(s_t, a_t) + \\gamma \\max_{a' \\in \\mathcal{A}} q_t(S_{t+1}, a') | S_t=s_t, A_t=a_t] - \\Upsilon_* q_t(s_t, a_t) \\\\\n",
    "&= \\Upsilon_* q_t(s_t, a_t) - \\Upsilon_* q_t(s_t, a_t) \\quad (\\because \\text{式(4.33)}) \\\\\n",
    "&= 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "$\\mathbb{E}[r_t | S_t=s_t, A_t=a_t] = \\mathbb{E}[R_t | S_t=s_t, A_t=a_t]$ \n",
    "* 二乗期待値が有界であることを示します。 $g(s, a)$ が有界であり、$q_t$ が有界であると仮定すると、$X_t(s_t, a_t)$ の二乗期待値も有界になります。具体的には、$g$ と $q_t$ の有界性から、\n",
    "$$\n",
    "\\mathbb{E}[X_t^2(s, a) | \\xi_t] \\leq c + d \\|q_t\\|^2_{\\infty}\n",
    "$$\n",
    "を満たす定数 $c, d \\ge 0$ が存在します。 $\\|q_t\\|_{\\infty} = \\max_{s,a} |q_t(s,a)|$ です。\n",
    "\n",
    "####  $Y_t$ の条件\n",
    "$Y_t(s, a) = 0$ と定義したので、任意の $\\beta_t \\rightarrow 0$ に対して\n",
    "$$\n",
    "|Y_t(s, a)| = 0 \\leq \\beta_t (\\|q_t\\|_{\\infty} + 1)\n",
    "$$\n",
    "が成り立ちます。\n",
    "\n",
    "#### 結論\n",
    "Q学習の更新則は、確率的近似における収束性のためのすべての条件を満たします。したがって、$q_t$ ($\\hat{Q}$) は、$\\Upsilon_*$ の唯一の不動点である$Q^*$ に確率1で収束します。\n",
    "\n",
    "$$\n",
    "\\lim_{t \\rightarrow \\infty} \\hat{Q}_t(s, a) = Q^*(s, a), \\quad \\forall (s, a) \\in \\mathcal{S} \\times \\mathcal{A} \\quad (\\text{w.p. } 1)\n",
    "$$\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
