{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9617b74a",
   "metadata": {},
   "source": [
    "# このページについて\n",
    "被覆数に関する勉強のまとめです．作成に当たっては，トン・ジャン先生の[機械学習のための数理解析](https://tongzhang-ml.org/)を大いに参考にさせていただきました．\n",
    "\n",
    "# 経験被覆数と一様被覆数\n",
    "真のデータ分布$D$に基づくような距離を使った被覆は$D$が既知でないと計算できません．通常真のデータ分布は未知であることがほとんどであり，真のデータ分布からサンプルされた確率変数の実現値$S_n = \\{Z_1, \\dots, Z_n\\}$が手元にあるはずです．ではそれをつかって被覆をしてみようというのが経験被覆数の概念です．\n",
    "\n",
    "## 経験$L_p$距離\n",
    "$S_n = \\{Z_1, \\dots, Z_n\\}$に基づく二つの関数の$L_p$距離$d = L_p(S_n)$を以下のように定義します．\n",
    "$$\n",
    "d(\\phi, \\phi') = \\left[ \\frac{1}{n} \\sum_{i=1}^n |\\phi(Z_i) - \\phi'(Z_i)|^p \\right]^{1/p}\n",
    "$$\n",
    "$p$は 1 以上の実数です．まあ，$L_{p}$ノルム的なやつですね　．この経験$L_p$距離$L_p(S_n)$を使って，仮説クラス$\\mathcal{G}$の**経験$L_p$被覆数**$N(\\epsilon, \\mathcal{G}, L_p(S_n))$を定義します．\n",
    "\n",
    "## 経験$L_p$被覆数\n",
    "訓練データ$S_n$と精度$\\epsilon > 0$に対して，経験$L_p$被覆数$N(\\epsilon, \\mathcal{G}, L_p(S_n))$とは，$\\mathcal{G}$内の任意の関数$\\phi$に対して，経験$L_p$距離が$\\epsilon$以下となるような代表関数$\\phi'$が$\\mathcal{G}_\\epsilon(\\cdot) \\subset V$(代表関数の集合) の中に必ず見つかるような，最小の集合$\\mathcal{G}_\\epsilon(\\cdot)$の要素数（濃度）のことです．\n",
    "\n",
    "経験$L_p$被覆数はデータに依存するため，任意のデータに対して収束を保証したい場合などには不便です．そこでデータへの依存性を排除するために，**一様$L_p$被覆数**を考えます．\n",
    "\n",
    "## 一様$L_p$被覆数\n",
    "サンプルサイズ$n$と精度$\\epsilon > 0$に対して，一様$L_p$被覆数$N_p(\\epsilon, \\mathcal{G}, n)$とは，サイズ$n$の任意のデータセット$S_n$に対して計算される経験$L_p$被覆数$N(\\epsilon, \\mathcal{G}, L_p(S_n))$の最大値を意味します．\n",
    "\n",
    "$$\n",
    "N_p(\\epsilon, \\mathcal{G}, n) = \\sup_{S_n:|S_n|=n} N(\\epsilon, \\mathcal{G}, L_p(S_n))\n",
    "$$\n",
    "次に，$p$の具体例として今後使うことになる**一様$L_1$被覆数$N_1(\\epsilon, \\mathcal{G}, n)$** と**一様$L_\\infty$被覆数$N_\\infty(\\epsilon, \\mathcal{G}, n)$** を考えましょう．後者は後に登場するVC次元と関係があります．その前に，異なる$p$の値に対する経験/一様$L_p$被覆数の関係を示します．\n",
    "## 経験/一様$L_p$被覆数の性質\n",
    "\n",
    "$1 \\le p \\le q$のとき，\n",
    "$$\n",
    "N(\\epsilon, \\mathcal{G}, L_p(S_n)) \\le N(\\epsilon, \\mathcal{G}, L_q(S_n))\n",
    "$$\n",
    "$$\n",
    "N_p(\\epsilon, \\mathcal{G}, n) \\le N_q(\\epsilon, \\mathcal{G}, n)\n",
    "$$\n",
    "が成り立ちます．経験被覆の大小関係は一様被覆でも保存されます．\n",
    "\n",
    "# 対称化\n",
    "仮説クラス全体に対する一様収束はPAC学習とユニオンバウンドを用いて行えることが第3章でわかりました．しかしそれは仮説クラスが有限な場合でした．もし仮説クラスが無限集合であったり，複雑な状況であった場合に別の方法を用いて評価を行うことを考えます．\n",
    "\n",
    "最初に，汎化ギャップを以下のように定義します．これは訓練誤差と，実際のテストでその関数を用いた場合の誤差の差分です．\n",
    "$$\n",
    "\\text{汎化ギャップ} = |err_D(\\hat{f}) - err_{S_n}(\\hat{f})|\n",
    "$$\n",
    "ただ，真のデータ分布はわかっていないので，そこからサンプルされた別のデータを用いて汎化ギャップを考え直しましょう．$S_n' = \\{Z_1', \\dots, Z_n'\\}$をそのようなデータとします．さらに検証誤差という$err_{S_n'}(f)$を定義します．現状の目標は，$|err_{S_n}(\\hat{f}) - err_{S_n'}(\\hat{f})|$を評価を評価することです．\n",
    "\n",
    "## 対称化経験過程\n",
    "経験過程を定義してこの目標にアタックしましょう．まず，経験誤差は，損失関数を$\\phi(f, Z_i)$と書くと，\n",
    "$$\n",
    "err_{S_n}(f) = \\frac{1}{n} \\sum_{i=1}^n \\phi(f, Z_i)\n",
    "$$\n",
    "検証誤差は，\n",
    "$$\n",
    "err_{S_n'}(f) = \\frac{1}{n} \\sum_{i=1}^n \\phi(f, Z_i')\n",
    "$$\n",
    "訓練誤差と検証誤差の差は，\n",
    "$$\n",
    "err_{S_n}(f) - err_{S_n'}(f) = \\frac{1}{n} \\sum_{i=1}^n (\\phi(f, Z_i) - \\phi(f, Z_i'))\n",
    "$$\n",
    "となります．最後に，ランダムな符号列$\\sigma = \\{\\sigma_1, \\dots, \\sigma_n\\}$を導入します。各$\\sigma_i$は独立に$\\pm 1$を確率$0.5$で取るとします。**対称化経験過程**は、以下のように定義されます。\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^n \\sigma_i \\phi(f, Z_i)\n",
    "$$\n",
    "なぜこれが，$err_{S_n}(f) - err_{S_n'}(f)$と関連するのでしょうか？対称化経験過程を導入する目的として，対称化補題が挙げられます．\n",
    "\n",
    "## 対称化補題\n",
    "実数値関数クラス$\\mathcal{F} = \\{f: \\mathcal{Z} \\to \\mathbb{R}\\}$を考える．ある関数$\\epsilon_n: (0, 1) \\to \\mathbb{R}$が存在して，確率少なくとも$1 - \\delta$で以下が成り立つと仮定する：\n",
    "$$\n",
    "\\forall f \\in \\mathcal{F}, \\quad \\frac{1}{n} \\sum_{i=1}^n \\sigma_i f(Z_i) \\le \\epsilon_n(\\delta)\n",
    "$$\n",
    "ここで，無作為性は訓練データ$S_n \\sim D^n$と符号$\\sigma$の両方にかかる．\n",
    "このとき，独立なランダムデータ$(S_n, S_n') \\sim D^{2n}$について，確率少なくとも$1 - \\delta$で以下が成り立つ：\n",
    "$$\n",
    "\\forall f \\in \\mathcal{F}, \\quad \\frac{1}{n} \\sum_{i=1}^n f(Z_i') \\le \\frac{1}{n} \\sum_{i=1}^n f(Z_i) + 2\\epsilon_n(\\delta/2)\n",
    "$$\n",
    "仮説クラス$\\mathcal{F}$内の$f$に対して，訓練データ$S_n$と検証データ$S_n'$の両方で検証誤差が訓練誤差よりも大きくずれる確率が小さい，ということです．では，これを用いて何を示していけばいいでしょうか．\n",
    "\n",
    "**汎化誤差の解析応用**\n",
    "1.  対称化補題を使って，訓練誤差$err_{S_n}(f)$と検証誤差$err_{S_n'}(f)$の差の上界を得る．\n",
    "2. $E[err_{S_n'}(f)] = err_D(f)$が成り立つ．\n",
    "3.  これらの関係を利用すると，最終的に訓練誤差$err_{S_n}(f)$からテスト誤差$err_D(f)$を評価するバウンドを導出することができる．\n",
    "\n",
    "# VC理論（Vapnik-Chervonenkis理論）\n",
    "VC理論は，二値分類問題における仮説クラスの複雑さを測るためのツールです．VC理論は，仮説クラスが無限であっても，クラスを測る有限な値（VC次元）を導入することで，汎化誤差の解析を可能にします．\n",
    "\n",
    "まず，二値分類を改めて定義します．\n",
    "入力空間$\\mathcal{X}$から出力空間$\\mathcal{Y} = \\{0, 1\\}$への関数$f: \\mathcal{X} \\to \\{0, 1\\}$を扱います．これはしばしば**概念 (コンセプト，concept)** と呼ばれます．学習の目標は，真の未知のコンセプト$f_*$を訓練データから学習することです．\n",
    "\n",
    "## シャッター (Shatter)\n",
    "ある仮説クラス$\\mathcal{C}$が，$n$個の異なるデータ点$S_n = \\{x_1, \\dots, x_n\\}$の集合をシャッター (shatter) するとは，$\\mathcal{C}$に含まれる関数を使えば，これらの$n$個の点に対する考えうるすべての$2^n$通りの二値ラベル付けを実現できることを意味します．\n",
    "\n",
    "集合$S_n = \\{x_1, \\dots, x_n\\}$と仮説クラス$\\mathcal{C}$が与えられたとき，$\\mathcal{C}$を$S_n$に制限した集合$\\mathcal{C}|_{S_n}$を考えます．これは，各関数$f \\in \\mathcal{C}$を$S_n$上での値のベクトル$(f(x_1), \\dots, f(x_n))$とみなした集合です．シャッターの定義は，この制限された集合の濃度$| \\mathcal{C}|_{S_n} |$が，$2^n$に等しいことと同値です．つまり，可能な二値ベクトルの全てを実現できるということです．\n",
    "\n",
    "$$\n",
    "|\\mathcal{C}|_{S_n}| = 2^n\n",
    "$$\n",
    "## VC次元の定義\n",
    "仮説クラス$\\mathcal{C}$のVC次元，$VC(\\mathcal{C})$と書くとは，$\\mathcal{C}$がシャッターできるデータ点集合の最大サイズ$n$のことです．もし，任意の数のデータ点集合をシャッターできるなら，VC次元は無限大であると定義されます．VC次元が大きければ大きいほど，仮説クラスは複雑であり，小さいほどそうではありません．\n",
    "\n",
    "## Sauerの補題\n",
    "仮説クラス$\\mathcal{G}$のVC次元が$d$であると仮定する．このとき，任意の$n > 0$と任意の$n$個のデータ点からなる集合$S_n = \\{Z_1, \\dots, Z_n\\} \\in \\mathcal{Z}^n$に対して，$\\mathcal{G}$を$S_n$に制限した集合の濃度$|\\mathcal{G}(S_n)|$は以下の不等式を満たす：\n",
    "$$\n",
    "|\\mathcal{G}(S_n)| \\le \\sum_{k=0}^d \\binom{n}{k} \\le \\max\\left(2, \\frac{en}{d}\\right)^d\n",
    "$$\n",
    "ここで，$\\binom{n}{k}$は二項係数です．この補題が示しているのは，VC次元$d$が有限である限り，データ点$n$が増えたとしても，仮説クラスがデータ上で実現できる異なる二値ラベル付けのパターン数$|\\mathcal{G}(S_n)|$は，最大でも$\\sum_{k=0}^d \\binom{n}{k}$で抑えられるということです．この値は$2^n$に比べて非常に小さな値らしく，$n$の多項式オーダーでしか増加しないという意味です．\n",
    "\n",
    "VC次元は，$L_\\infty$経験被覆数の上界を示します．Sauerの補題そのものが，$L_\\infty$距離で$\\epsilon=0$とした場合の経験被覆数$|\\mathcal{G}(S_n)| = N(0, \\mathcal{G}, L_\\infty(S_n))$に上限を与えています．そして，一般の$\\epsilon > 0$や$p < \\infty$の経験$L_p$被覆数についても，VC次元を用いて上界を評価することができます．例えば，一様$L_1$被覆数はVC次元$d$を用いて以下のようにバウンドできます．\n",
    "$$\n",
    "\\ln N_1(\\epsilon, \\mathcal{G}, n) \\le C \\left( d \\ln\\left(\\frac{1}{\\epsilon}\\right) + \\dots \\right)\n",
    "$$\n",
    "ここで$C$は定数で，$...$は$d$や$n$に依存する対数項などを含みます．\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
