{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## はじめに\n",
    "確率不等式をまとめていきます。どういった条件で使えるかなど実用を考慮したまとめです。作成に当たっては、トン・ジャン先生の[機械学習のための数理解析](https://tongzhang-ml.org/)を大いに参考にさせていただきました。先生、ありがとうございます。\n",
    "\n",
    "\n",
    "## 確率不等式\n",
    "\n",
    "### 変数定義\n",
    "$X_1, \\ldots, X_n$ ：独立同分布の確率変数\n",
    "$\\mu=\\mathbb{E}\\left[X_i\\right]$ ：共通の期待値\n",
    "$\\bar{X}_n=\\frac{1}{n} \\sum_{i=1}^n X_i:$ 標本平均\n",
    "### マルコフファミリー\n",
    "#### マルコフの不等式(一般化)\n",
    "任意の$h(x) \\geq 0$と任意の集合$S \\subset \\mathbb{R}$に対して、以下の不等式が成り立ちます。\n",
    "$$\n",
    "\\operatorname{Pr}\\left(\\bar{X}_n \\in S\\right) \\leq \\frac{\\mathbb{E}\\left[h\\left(\\bar{X}_n\\right)\\right]}{\\inf _{x \\in S} h(x)}\n",
    "$$\n",
    "#### 証明\n",
    "\n",
    "$\\mathbb{E}\\left[h\\left(\\bar{X}_n\\right)\\right]=\\mathbb{E}\\left[h\\left(\\bar{X}_n\\right) \\cdot 1_{\\bar{X}_n \\in S}\\right]+\\mathbb{E}\\left[h\\left(\\bar{X}_n\\right) \\cdot 1_{\\bar{X}_n \\notin S}\\right] \\geq \\mathbb{E}\\left[h\\left(\\bar{X}_n\\right) \\cdot 1_{\\bar{X}_n \\in S}\\right]$\n",
    "\n",
    "$\\bar{X}_n \\in S$のとき、$h\\left(\\bar{X}_n\\right)$は自明に、$\\inf _{x \\in S} h(x)$より大きいので、上右辺以下の式が導けます。\n",
    "\n",
    "$\\mathbb{E}\\left[h\\left(\\bar{X}_n\\right) \\cdot 1_{\\bar{X}_n \\in S}\\right] \\geq \\inf _{x \\in S} h(x) \\cdot \\mathbb{E}\\left[1_{\\bar{X}_n \\in S}\\right]=\\inf _{x \\in S} h(x) \\cdot \\operatorname{Pr}\\left(\\bar{X}_n \\in S\\right)$\n",
    "\n",
    "ここで、第2項について、期待値は確率変数の実現値と確率の内積ですが、実現値は$\\bar{X}_n \\in S$のみ1なので、それに対応する確率$\\operatorname{Pr}\\left(\\bar{X}_n \\in S\\right)$がでてきます。\n",
    "\n",
    "よって、\n",
    "\n",
    "$\\mathbb{E}\\left[h\\left(\\bar{X}_n\\right)\\right] \\geq \\inf _{x \\in S} h(x) \\cdot \\operatorname{Pr}\\left(\\bar{X}_n \\in S\\right)$\n",
    "\n",
    "式変形をして、\n",
    "\n",
    "$\\operatorname{Pr}\\left(\\bar{X}_n \\in S\\right) \\leq \\frac{\\mathbb{E}\\left[h\\left(\\bar{X}_n\\right)\\right]}{\\inf _{x \\in S} h(x)}$\n",
    "\n",
    "となります。\n",
    "\n",
    "#### 具体例：マルコフの不等式(よく見るやつ)\n",
    "\n",
    "一般的な式ではわかりにくいので、\n",
    "* $h(x)=x$\n",
    "* $S=[a, \\infty)$ ：平均が $a$ 以上になる確率\n",
    "としましょう。このとき、マルコフの不等式は、\n",
    "$$\n",
    "\\operatorname{Pr}\\left(\\bar{X}_n \\geq a\\right) \\leq \\frac{\\mathbb{E}\\left[\\bar{X}_n\\right]}{a}\n",
    "$$\n",
    "となり、よく知られたマルコフの不等式の形になります。\n",
    "\n",
    "#### 具体例：チェビシェフの不等式\n",
    "\n",
    "分散が分かっている場合、マルコフの不等式はもう少しタイトにできます。\n",
    "* $h(x)=(x-\\mu)^2$ (分散の破片)\n",
    "* $S=\\{x:|x-\\mu| \\geq \\varepsilon\\}$\n",
    "としましょう。そうすると、\n",
    "$$\n",
    "\\operatorname{Pr}\\left(\\left|\\bar{X}_n-\\mu\\right| \\geq \\varepsilon\\right) \\leq \\frac{\\mathbb{E}\\left[\\left(\\bar{X}_n-\\mu\\right)^2\\right]}{\\varepsilon^2}=\\frac{\\operatorname{Var}\\left(\\bar{X}_n\\right)}{\\varepsilon^2}=\\frac{\\sigma^2 / n}{\\varepsilon^2}=\\frac{\\sigma^2}{n \\varepsilon^2}\n",
    "$$\n",
    "となり、これがチェビシェフの不等式です。\n",
    "* 分散が大きかったり、確率の尾部が大きかったりすると上界が高くなってしまい、そこまでタイトになりません。\n",
    "* 分散が少ないと、経験平均が真の平均から離れる確率は低くなるので、タイトになります。\n",
    "\n",
    "#### 具体例：指数型マルコフの不等式\n",
    "\n",
    "確率変数が大きな値をとる場合に使えるのが指数型マルコフの不等式です。\n",
    "* $h(x)=e^{\\lambda x}$\n",
    "*  $S=[a, \\infty)$\n",
    "としましょう。そうすると、\n",
    "$$\n",
    "\\operatorname{Pr}(X \\geq a)=\\operatorname{Pr}(X \\in[a, \\infty)) \\leq \\frac{\\mathbb{E}\\left[e^{\\lambda X}\\right]}{\\inf _{x \\geq a} e^{\\lambda x}}=\\frac{\\mathbb{E}\\left[e^{\\lambda X}\\right]}{e^{\\lambda a}}\n",
    "$$\n",
    "となります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upper bound\n",
    "対数モーメント母関数を定義して、rate functionにブチコむ\n",
    "rate functionを考える。これはルジャンドル変換と同じっぽい\n",
    "そうすると、確率変数の経験平均が実際の平均＋εより大きくなる確率の上界を得ることができる。\n",
    "証明は簡単\n",
    "\n",
    "#### Lower bound\n",
    "large deviation stuationを考える。経験平均が実際の平均より大きくなってしまう場合を考える。\n",
    "rate functionは最強\n",
    "$\\Lambda_X(0)|_{\\lambda = 0} = 0$\n",
    "左の関数において、縦線の右下にいる条件が成立する場合は右辺が関数の出力となる。\n",
    "モーメント母関数をλ=0で一回微分すると、Xの期待値、二階微分するとXの分散がでてくる。\n",
    "\n",
    "\n",
    "#### サブガウシアン\n",
    "サブガウシアンとは、確率変数がとる値の尾部の減少の速度が、ガウス分布と同等かそれ以上になるので、モーメント母関数に対して上界が成り立つ。モーメント母関数に対して成り立つとなぜ減少速度とかの話になるのかが疑問\n",
    "#### サブガウシアンに対して成り立つ尾部不等式\n",
    "経験平均が実際の平均よりε大きくなる確率は上界で抑えられるかつ、εが大きくなるほど、その確率はどんどん減っていく。\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text { If } X_1 \\text { is sub-Gaussian as in (3), then for all } \\epsilon>0 \\text { : }\\\\\n",
    "&\\begin{aligned}\n",
    "& \\operatorname{Pr}\\left(\\bar{X}_n \\geq \\mu+\\epsilon\\right) \\leq e^{-n \\epsilon^2 / 2 b} \\\\\n",
    "& \\operatorname{Pr}\\left(\\bar{X}_n \\leq \\mu-\\epsilon\\right) \\leq e^{-n \\epsilon^2 / 2 b} .\n",
    "\\end{aligned}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### 確率不等式の変換\n",
    "以下のようなよく見る形に変換できます。\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text { With probability at least } 1-\\delta \\text {, we have }\\\\\n",
    "&\\bar{X}_n<\\mu+\\sqrt{\\frac{2 b \\ln (1 / \\delta)}{n}} .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### サブガウシアン以外に対して成り立つ尾部不等式\n",
    "\n",
    "If $X_1$ has a logarithmic moment generating function that satisfies (4) for $\\lambda>0$, then all $\\epsilon>0$ :\n",
    "\n",
    "$$\n",
    "\\operatorname{Pr}\\left(\\bar{X}_n \\geq \\mu+\\epsilon\\right) \\leq \\exp \\left[\\frac{-n \\epsilon^2}{2(\\alpha+\\beta \\epsilon)}\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "Moreover, for $t>0$, we have\n",
    "\n",
    "$$\n",
    "\\operatorname{Pr}\\left(\\bar{X}_n \\geq \\mu+\\sqrt{\\frac{2 \\alpha t}{n}}+\\frac{\\beta t}{n}\\right) \\leq e^{-t}\n",
    "$$\n",
    "#### チェルノフorヘフディングの尾部不等式\n",
    "確率変数の大きさが0から1でバウンドされている場合の確率不等式である。報酬関数とかについて出せそう？\n",
    "**加法的**\n",
    "* 絶対誤差でとっている。\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text { Assume that } X_1 \\in[0,1] \\text {. Then for all } \\epsilon>0 \\text { : }\\\\\n",
    "&\\begin{aligned}\n",
    "& \\operatorname{Pr}\\left(\\bar{X}_n \\geq \\mu+\\epsilon\\right) \\leq e^{-2 n \\epsilon^2} \\\\\n",
    "& \\operatorname{Pr}\\left(\\bar{X}_n \\leq \\mu-\\epsilon\\right) \\leq e^{-2 n \\epsilon^2}\n",
    "\\end{aligned}\n",
    "\\end{aligned}\n",
    "$$\n",
    "**乗法的**\n",
    "* 相対誤差でとっている。\n",
    "* 緩いけど$\\mu$が小さい場合に扱いやすいらしい\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text { Assume that } X_1 \\in[0,1] \\text {. Then for all } \\epsilon>0 \\text { : }\\\\\n",
    "&\\begin{aligned}\n",
    "& \\operatorname{Pr}\\left(\\bar{X}_n \\geq(1+\\epsilon) \\mu\\right) \\leq \\exp \\left[\\frac{-n \\mu \\epsilon^2}{2+\\epsilon}\\right], \\\\\n",
    "& \\operatorname{Pr}\\left(\\bar{X}_n \\leq(1-\\epsilon) \\mu\\right) \\leq \\exp \\left[\\frac{-n \\mu \\epsilon^2}{2}\\right] .\n",
    "\\end{aligned}\n",
    "\\end{aligned}\n",
    "$$\n",
    "#### ベネットの尾部不等式\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### バーンスタインの不等式\n",
    "* 分散が入っているのでチェルノフの不等式よりタイト\n",
    "* 分散が小さいと、経験平均は真の平均に近くなる。つまり、真の平均からε離れる確率は、分散が大きく、経験平均が真の平均から離れる場合より低くなる。\n",
    "\n",
    "\n",
    "#### Non IIDの場合\n",
    "* iidの場合、期待値はすべて同じなので$n$回かければいいが、独立非同分布の場合、期待値が毎回違う。\n",
    "\n",
    "#### サブガウシアンバウンド\n",
    "$\\ln \\mathbb{E}\\left[e^{\\lambda X_i}\\right] \\leq \\lambda \\mathbb{E}\\left[X_i\\right]+\\frac{\\lambda^2 b_i}{2}$\n",
    "$\\operatorname{Pr}\\left(\\bar{X}_n \\geq \\mu+\\varepsilon\\right) \\leq \\exp \\left(-\\frac{n^2 \\varepsilon^2}{2 \\sum_{i=1}^n b_i}\\right)$\n",
    "は、iidの場合、Σの部分が$nb$になる。ので、分子の$n^2$の$n$が消える。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
