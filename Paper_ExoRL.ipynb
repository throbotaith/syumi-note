{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample-Efficient Reinforcement Learning in the Presence of Exogenous Information \n",
    "\n",
    "Yonathan Efroni ${ }^{1}$, Dylan J. Foster ${ }^{1}$, Dipendra Misra ${ }^{1}$, Akshay Krishnamurthy ${ }^{1}$, and John Langford ${ }^{1}$<br>${ }^{1}$ Microsoft Research NYC\n",
    "\n",
    "\n",
    "RLを実世界で扱う場合、高次元のデータを入力として扱う場合があります。高次元のデータにはタスクに関連した情報や全然関係のないような情報が含まれることがあります。この研究ではそのような状況におけるサンプル効率の良い強化学習アルゴリズムを提案しました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 表現学習とRLの関係(関連研究)\n",
    "* 高次元入力から関連する情報を抽出する研究は機械学習や統計学で多く研究されてきました。しかしRLにこれらの技術を単純に拡張するだけでは不十分であると思われます（Gelada et al., 2019; Zhang et al., 2020）。\n",
    "* しかし近年、高次元入力のRLに対して、統計的に扱いやすくなるような条件の研究がいくつか行われました（Jiang et al., 2017; Jin et al., 2021; Du et al., 2021; Foster et al., 2021, Du et al., 2019; Misra et al., 2020; Agarwal et al., 2020; Misra et al., 2021; Uehara et al., 2021）。\n",
    "* これらの研究はいい感じなんですが、**時間的に相関があるが、意思決定と関係ない外生的情報を扱うことが難しい**です。例えば、歩いているときに上空を同じ速度で飛ぶ鳥は時間的に相関がありますが、意思決定には関係ありません。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 本研究の構造\n",
    "* 動的かつタスクに対して無関係な特徴量(外生情報)を含む高次元入力に対するRLの研究（Efroni et al. (2021b)）が基になっています。\n",
    "* 本研究では外生MDPおよび、ExoRLを新しく提案しました。無関係な特徴量に依存しないnear-optimal方策を学習させるアルゴリズムです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MDP\n",
    "本研究では有限Horizon MDPを採用したようです。$$\\mathcal{M}=\\left(\\mathcal{S}, \\mathcal{A}, T, R, H, d_{1}\\right)$$\n",
    "\n",
    "$\\mathcal{S}$：状態空間 \\\n",
    "$\\mathcal{A}$：行動空間 \\\n",
    "$T: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\Delta(\\mathcal{S})$：遷移カーネル \\\n",
    "$R: \\mathcal{S} \\times \\mathcal{A} \\rightarrow[0,1]$：報酬関数 \\\n",
    "$H \\in \\mathbb{N}$：Horizon \\\n",
    "$d_{1} \\in \\Delta(\\mathcal{S})$：初期状態分布 \\\n",
    "$\\pi=\\left(\\pi_{1}, \\ldots, \\pi_{H}\\right)$：非定常方策 \\\n",
    "$J(\\pi)=\\mathbb{E}_{\\pi}\\left[\\sum_{h=1}^{H} r_{h}\\right]$：平均報酬 \n",
    "\n",
    "最適化目標\n",
    "* $J(\\widehat{\\pi}) \\geq \\max _{\\pi \\in \\Pi_{\\mathrm{NS}}} J(\\pi)-\\epsilon$となる方策$\\widehat{\\pi}$を獲得することが目標($\\epsilon$-最適方策を学習すること)です。\n",
    "* $\\Pi_{\\mathrm{NS}}$：全ての非定常方策$\\pi=\\left(\\pi_{1}, \\ldots, \\pi_{H}\\right)$の集合\n",
    "\n",
    "#### ExoMDP\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
