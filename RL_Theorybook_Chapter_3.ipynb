{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning: Theory and Algorithms\n",
    "\n",
    "* 第3章における各定理、補題、系などをまとめます。証明は初見でもこれを参考にすれば導出できるように丁寧に記します。\n",
    "* 必要に応じて実装を行います。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Bellman Completeness(LBC) \n",
    "\n",
    "* 状態数と行動数が大きい場合(可算あるいは非可算無限)場合にうまく動くアルゴリズムを考えます。\n",
    "* LBCはある条件であり、これを満たすと多項式的なサンプル複雑度で最適な方策を学習できます。\n",
    "* 本章では**有限-Horizon**MDPに焦点を当てます。\n",
    "\n",
    "\n",
    "#### 準備\n",
    "* 線形関数：$f(s, a) := \\theta^{\\top} \\phi(s, a)$\n",
    "* 特徴写像：$\\phi: \\mathcal{S} \\times \\mathcal{A} \\mapsto \\mathbb{R}^d$\n",
    "* 適切な次元の行列 $M$ とベクトル $x$ に対して、$\\|x\\|_M^2 = x^{\\top} M x$ という表記を使用します．\n",
    "\n",
    "\n",
    "#### Linear Bellman Completeness\n",
    "\n",
    "特徴 $\\phi$ がLinear Bellman Completenessを満たすとは、任意の $\\theta \\in \\mathbb{R}^d$ および $(s, a, h) \\in \\mathcal{S} \\times \\mathcal{A} \\times[H]$ に対して、次を満たす $w \\in \\mathbb{R}^d$ が存在することをいいます。\n",
    "\n",
    "$$\n",
    "w^{\\top} \\phi(s, a) = r(s, a) + \\mathbb{E}_{s^{\\prime} \\sim P_h(s, a)} \\max_{a^{\\prime}} \\theta^{\\top} \\phi(s^{\\prime}, a^{\\prime}).\n",
    "$$\n",
    "\n",
    "$w$ は $\\theta$ に依存するため、上記の方程式で $w := \\mathcal{T}_h(\\theta)$ と表記します。\n",
    "\n",
    "* 線形関数にベルマン作用素を適用($w^{\\top} \\phi(s, a) = \\mathcal{T}_h(\\theta)\\phi(s, a)$)しても、適用結果が線形関数で表現できる(右辺)という条件です。つまり、線形関数にベルマン作用素を適用しても非線形関数とかにならないという条件です。\n",
    "* 線形であることのメリットとして、$Q_h^{\\star}(s, a) = (\\theta_h^{\\star})^{\\top} \\phi(s, a)$ を満たす $\\theta_h^{\\star}$ が存在することが挙げられます。つまりLBCを満たせば、最適価値関数は必ず存在するということです。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSVIアルゴリズム\n",
    "最適価値関数$Q_h^{\\star}(s, a)$を学習するアルゴリズムを考えましょう．\n",
    "\n",
    "\n",
    "アルゴリズム 1 最小二乗値反復 (LSVI)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* LSVIがほぼ最適な方策を見つけるためには，**データセット**をいい感じにする必要があります．\n",
    "* いい感じにするには，各データセット$\\mathcal{D}_h$において，$\\sum_{s, a \\in \\mathcal{D}_h} \\phi(s, a) \\phi(s, a)^{\\top}$が**フルランク**になっている必要があります．\n",
    "  * これにより，最小二乗法を閉じた形で解くことができ，いい感じに汎化することができます．\n",
    "  * なぜフルランクになると閉じた形になるかというと(**TODO**)\n",
    "* この条件(フルランク)を達成するデータセットを構成するために，**D-最適設計**を使います．\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D最適設計\n",
    "* 特徴集合全体をスペクトル(固有値の集合？)的な意味でいい感じにカバーできるサンプリング分布を以下のように定義します．\n",
    "\n",
    "**定理 3.2** $\\mathcal{X} \\subset \\mathbb{R}^d$ がコンパクト集合（かつフル次元）であると仮定する。このとき、次の特性を持つ $\\mathcal{X}$ 上の分布 $\\rho$ が存在し，**D-最適設計**という．\n",
    "\n",
    "\n",
    "\n",
    "* 特性①： $\\rho$ は、最大 $d(d+1)/2$ 個の点（すべて $\\mathcal{X}$ 内にある）でサポートされています．\n",
    "  * サポートとは，その点での確率が０より大きい値を取る集合のことです．\n",
    "  * コンパクト集合とは，**TODO**\n",
    "\n",
    "* 特性②：$\\Sigma = \\mathbb{E}_{x \\sim \\rho} [xx^{\\top}]$とした時，任意の$x \\in \\mathcal{X}$ に対して次が成り立ちます．\n",
    "\n",
    "$$\\|x\\|_{\\Sigma^{-1}}^2 \\leq d$$\n",
    "\n",
    "  * 直感的に見ると，$x \\in \\mathcal{X}$のノルムに上界が与えられているので，ベクトルの大きさが一定の大きさ以下であるということですね．\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
