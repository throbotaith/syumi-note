{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69bb9c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final policy (flattened) from S‑EpiRC‑PGS:\n",
      "[0.84303778 0.15696222 0.99069685 0.00930315 1.         0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_23308\\264637233.py:702: UserWarning: constrained_layout not applied because axes sizes collapsed to zero.  Try making figure larger or Axes decorations smaller.\n",
      "  fig.savefig('learning_results.png')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning curves and final policy visualisations have been saved as PNG files.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implementation of the stochastic Epigraph Robust Constrained Policy Gradient Search\n",
    "(S‑EpiRC‑PGS).\n",
    "\n",
    "This module provides a simple reference implementation of the algorithm\n",
    "presented in Kitamura et al. (2025).  In contrast to the deterministic\n",
    "version of EpiRC‑PGS contained in ``epirc-pgs.py``, the stochastic\n",
    "counterpart replaces exact gradient and value computations with Monte\n",
    "Carlo sampling.  The inner loop therefore uses a REINFORCE style\n",
    "policy gradient estimator, while the outer loop performs a noisy\n",
    "bisection search based on the PBA (probabilistic bisection algorithm).\n",
    "\n",
    "Key features of this implementation:\n",
    "  * An uncertainty set of transition kernels ``U`` is generated\n",
    "    randomly.  The user may swap the ``make_U`` function for one\n",
    "    generating a KL‑ball of kernels if desired.  Each element of ``U``\n",
    "    is a three‑dimensional array of shape ``(S, A, S)`` encoding\n",
    "    conditional next‑state probabilities.\n",
    "  * Cost functions for the objective and constraints are drawn at\n",
    "    random.  The thresholds for the constraints (entries 1–N‑1 of\n",
    "    ``B``) are sampled uniformly from the effective horizon range.\n",
    "  * The inner loop (Algorithm 2 in the paper) applies a projection\n",
    "    step after each gradient update to ensure the policy remains a\n",
    "    valid probability distribution for each state.  The gradient is\n",
    "    estimated using the REINFORCE trick: for the worst case cost\n",
    "    component the algorithm samples a trajectory from the worst\n",
    "    environment and uses discounted returns to construct an unbiased\n",
    "    estimator of the gradient of the expected cost.\n",
    "  * The outer loop (Algorithm 1) implements a discrete version of\n",
    "    PBA.  A finite grid over the search interval ``[0,H]`` defines a\n",
    "    belief distribution over the optimal cost.  At each iteration the\n",
    "    median of this belief determines the test threshold ``b0``.  The\n",
    "    sign of the maximum constraint violation returned by the inner\n",
    "    loop updates the belief, favouring the half interval suggested by\n",
    "    the outcome.  A reliability parameter ``p > 0.5`` encodes the\n",
    "    probability that the inner loop’s answer is correct; following\n",
    "    Waeber et al. (2013) the belief update multiplies the weights on\n",
    "    one side of ``b0`` by ``p`` and the other side by ``1‑p``.\n",
    "\n",
    "This code is intended for educational purposes and has been kept\n",
    "compact rather than optimised for speed.  For small state and action\n",
    "spaces it should run in a reasonable amount of time.  To port the\n",
    "algorithm to more complex environments (e.g. with function\n",
    "approximation) one would need to extend the sampling and gradient\n",
    "estimation routines appropriately.\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------- Global hyper‑parameters -------------------------\n",
    "\n",
    "# Number of states and actions in the tabular MDP\n",
    "S = 3  # 状態数\n",
    "A = 2  # 行動数\n",
    "\n",
    "# Number of cost functions (one objective + constraints)\n",
    "N = 4\n",
    "\n",
    "# Number of uncertainty kernels in the uncertainty set U\n",
    "U_num = 3\n",
    "\n",
    "# Discount factor and effective horizon\n",
    "gamma = 0.95\n",
    "H = int(np.round(1 / (1 - gamma)))  # 有効地平線\n",
    "\n",
    "# Learning rate for the inner loop\n",
    "alpha = 0.01\n",
    "\n",
    "# Number of iterations in the inner loop (per call of Algorithm 2)\n",
    "T_inner = 50\n",
    "\n",
    "# Number of iterations of the outer loop (Algorithm 1)\n",
    "K_outer = 100\n",
    "\n",
    "# Reliability parameter for PBA (must be > 0.5).  A higher value\n",
    "# implies more trust in the sign returned by the inner loop.\n",
    "p_reliability = 0.7\n",
    "\n",
    "# Resolution of the belief grid over [0, H] used in PBA.  A finer\n",
    "# grid yields a more accurate estimate of the optimal value but\n",
    "# increases computational cost.\n",
    "belief_grid_size = 101\n",
    "\n",
    "# Initial policy π is a random stochastic matrix of shape (S,A)\n",
    "pi_table = np.random.rand(S, A)\n",
    "pi_table = pi_table / np.sum(pi_table, axis=1, keepdims=True)\n",
    "pi_flat = pi_table.flatten()\n",
    "\n",
    "# Random initial state distribution μ (length S)\n",
    "mu = np.random.rand(S)\n",
    "mu = mu / np.sum(mu)\n",
    "\n",
    "\n",
    "# ------------------------- Uncertainty set and costs -------------------------\n",
    "\n",
    "def make_U():\n",
    "    \"\"\"\n",
    "    Generate a list of transition kernels forming the uncertainty set.\n",
    "\n",
    "    By default this function samples each kernel independently from a\n",
    "    uniform Dirichlet distribution over successor states.  To\n",
    "    construct a KL‑ball around a nominal kernel one could modify this\n",
    "    routine accordingly.\n",
    "    \"\"\"\n",
    "    U_list = []\n",
    "    for _ in range(U_num):\n",
    "        P_kernel = np.random.rand(S, A, S)\n",
    "        P_kernel = P_kernel / np.sum(P_kernel, axis=2, keepdims=True)\n",
    "        U_list.append(P_kernel)\n",
    "    return U_list\n",
    "\n",
    "\n",
    "def make_C_and_B():\n",
    "    \"\"\"\n",
    "    Create random cost matrices C_n(s,a) and thresholds B_n.\n",
    "\n",
    "    For each cost (objective + constraints) we draw a random\n",
    "    non‑negative cost matrix.  For the objective (index 0) the\n",
    "    threshold is placeholder and will be set by the outer loop.  For\n",
    "    the constraints we draw integer thresholds uniformly between 0\n",
    "    and H inclusive.\n",
    "    \"\"\"\n",
    "    C_list = []\n",
    "    B_list = []\n",
    "    for n in range(N):\n",
    "        c_sa = np.random.rand(S, A)\n",
    "        C_list.append(c_sa)\n",
    "        if n == 0:\n",
    "            B_list.append(0.0)  # placeholder for objective\n",
    "        else:\n",
    "            B_list.append(float(random.randint(0, H)))\n",
    "    return C_list, B_list\n",
    "\n",
    "\n",
    "# Instantiate the uncertainty set and cost functions\n",
    "U = make_U()\n",
    "C, B = make_C_and_B()\n",
    "\n",
    "\n",
    "# ------------------------- Projection utilities -------------------------\n",
    "\n",
    "def projection_to_simplex(y_vec: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Project a vector onto the probability simplex (non‑negative entries\n",
    "    summing to one).\n",
    "\n",
    "    This helper implements the algorithm of Duchi et al. (2008).  It\n",
    "    is used to ensure each row of the policy table remains a valid\n",
    "    probability distribution after the gradient update.\n",
    "\n",
    "    Args:\n",
    "        y_vec (np.ndarray): vector of shape (A,) to project\n",
    "    Returns:\n",
    "        np.ndarray: projected vector of shape (A,)\n",
    "    \"\"\"\n",
    "    m = y_vec.shape[0]\n",
    "    # Sort entries in descending order\n",
    "    u = np.sort(y_vec)[::-1]\n",
    "    cssv = np.cumsum(u)\n",
    "    rho = -1\n",
    "    for i in range(m):\n",
    "        if u[i] + (1 - cssv[i]) / (i + 1) > 0:\n",
    "            rho = i\n",
    "    if rho == -1:\n",
    "        return np.ones(m) / m\n",
    "    theta = (cssv[rho] - 1) / (rho + 1)\n",
    "    return np.maximum(y_vec - theta, 0)\n",
    "\n",
    "\n",
    "def proj_policy_matrix(policy_matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Project each row of a policy matrix onto the simplex.\n",
    "\n",
    "    Args:\n",
    "        policy_matrix (np.ndarray): array of shape (S,A)\n",
    "    Returns:\n",
    "        np.ndarray: projected matrix of shape (S,A)\n",
    "    \"\"\"\n",
    "    projected = np.zeros_like(policy_matrix)\n",
    "    for s_idx in range(policy_matrix.shape[0]):\n",
    "        projected[s_idx, :] = projection_to_simplex(policy_matrix[s_idx, :])\n",
    "    return projected\n",
    "\n",
    "\n",
    "def update_and_project_policy(current_policy_table: np.ndarray,\n",
    "                              gradient_table: np.ndarray,\n",
    "                              lr: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform a gradient descent step followed by projection onto the\n",
    "    probability simplex for each state.\n",
    "\n",
    "    Args:\n",
    "        current_policy_table (np.ndarray): current policy matrix of\n",
    "            shape (S,A)\n",
    "        gradient_table (np.ndarray): gradient estimate of shape (S,A)\n",
    "        lr (float): learning rate\n",
    "    Returns:\n",
    "        np.ndarray: updated and projected policy matrix of shape (S,A)\n",
    "    \"\"\"\n",
    "    if current_policy_table.shape != gradient_table.shape:\n",
    "        raise ValueError(\n",
    "            \"Shape mismatch between current_policy_table and gradient_table.\")\n",
    "    y = current_policy_table - lr * gradient_table\n",
    "    return proj_policy_matrix(y)\n",
    "\n",
    "\n",
    "# ------------------------- Value computation (exact) -------------------------\n",
    "\n",
    "def compute_Q_pi_c_P_matrix(P_kernel: np.ndarray,\n",
    "                            Q_initial: np.ndarray,\n",
    "                            cost_sa: np.ndarray,\n",
    "                            pi_flat: np.ndarray,\n",
    "                            max_iterations: int = 1000,\n",
    "                            tolerance: float = 1e-3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Solve for the Q‑function Q^π(s,a) under fixed P and cost c via\n",
    "    successive approximation of the Bellman equation.  This function\n",
    "    mirrors the deterministic helper used in ``epirc-pgs.py``.\n",
    "\n",
    "    Args:\n",
    "        P_kernel (np.ndarray): transition kernel of shape (S,A,S)\n",
    "        Q_initial (np.ndarray): initial guess for Q, shape (S,A)\n",
    "        cost_sa (np.ndarray): cost matrix, shape (S,A)\n",
    "        pi_flat (np.ndarray): flattened policy of length S*A\n",
    "        max_iterations (int): maximum number of iterations\n",
    "        tolerance (float): convergence tolerance\n",
    "    Returns:\n",
    "        np.ndarray: converged Q table of shape (S,A)\n",
    "    \"\"\"\n",
    "    Q_k = np.copy(Q_initial)\n",
    "    for iteration in range(max_iterations):\n",
    "        Q_k_plus = np.zeros_like(Q_k)\n",
    "        # Precompute V^π(s) for all s under current Q_k\n",
    "        # Flatten policy into table for convenience\n",
    "        pi_matrix = pi_flat.reshape(S, A)\n",
    "        V = np.zeros(S)\n",
    "        for s_idx in range(S):\n",
    "            V[s_idx] = np.dot(pi_matrix[s_idx, :], Q_k[s_idx, :])\n",
    "        # Perform Bellman update\n",
    "        for s_idx in range(S):\n",
    "            for a_idx in range(A):\n",
    "                expected_future = np.dot(P_kernel[s_idx, a_idx, :], V)\n",
    "                Q_k_plus[s_idx, a_idx] = cost_sa[s_idx, a_idx] + gamma * expected_future\n",
    "        # Check convergence\n",
    "        if np.max(np.abs(Q_k_plus - Q_k)) < tolerance:\n",
    "            return Q_k_plus\n",
    "        Q_k = Q_k_plus\n",
    "    return Q_k\n",
    "\n",
    "\n",
    "def compute_J_c_P(P_kernel: np.ndarray,\n",
    "                  Q_initial: np.ndarray,\n",
    "                  cost_sa: np.ndarray,\n",
    "                  pi_flat: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute the expected discounted cost J_{c,P}(π) = μ^T V^π for a\n",
    "    given transition kernel P and cost matrix c.\n",
    "\n",
    "    Args:\n",
    "        P_kernel (np.ndarray): transition kernel of shape (S,A,S)\n",
    "        Q_initial (np.ndarray): initial guess for Q, shape (S,A)\n",
    "        cost_sa (np.ndarray): cost matrix, shape (S,A)\n",
    "        pi_flat (np.ndarray): flattened policy of length S*A\n",
    "    Returns:\n",
    "        float: expected discounted cost under μ\n",
    "    \"\"\"\n",
    "    Q_sa = compute_Q_pi_c_P_matrix(P_kernel, Q_initial, cost_sa, pi_flat)\n",
    "    pi_matrix = pi_flat.reshape(S, A)\n",
    "    V = np.zeros(S)\n",
    "    for s_idx in range(S):\n",
    "        V[s_idx] = np.dot(pi_matrix[s_idx, :], Q_sa[s_idx, :])\n",
    "    return float(np.dot(mu, V))\n",
    "\n",
    "\n",
    "def compute_J_c_U_b_and_its_max(U_list, C_list, Q_initial_list, B_list, pi_flat):\n",
    "    \"\"\"\n",
    "    For each cost index n, compute the worst case cost over all\n",
    "    environment kernels and subtract the corresponding threshold.\n",
    "\n",
    "    Args:\n",
    "        U_list (list): list of transition kernels\n",
    "        C_list (list): list of cost matrices c_n\n",
    "        Q_initial_list (list): list of initial Q matrices\n",
    "        B_list (list): list of thresholds b_n\n",
    "        pi_flat (np.ndarray): current policy\n",
    "    Returns:\n",
    "        tuple: (J_results, J_max_index, J_max_index_U) where\n",
    "            J_results[n] = max_i J_{c_n,U_i}(π) - B[n]\n",
    "            J_max_index is the index n of the largest violation\n",
    "            J_max_index_U[n] is the environment index achieving the\n",
    "            maximum for cost n\n",
    "    \"\"\"\n",
    "    J_results = []\n",
    "    J_max_index_U = []\n",
    "    for n in range(N):\n",
    "        values = []\n",
    "        for i in range(U_num):\n",
    "            val = compute_J_c_P(U_list[i], Q_initial_list[n], C_list[n], pi_flat)\n",
    "            values.append(val)\n",
    "        worst_env = int(np.argmax(values))\n",
    "        J_max_index_U.append(worst_env)\n",
    "        J_results.append(values[worst_env] - B_list[n])\n",
    "    J_max_index = int(np.argmax(J_results))\n",
    "    return J_results, J_max_index, J_max_index_U\n",
    "\n",
    "\n",
    "def compute_delta(pi_flat: np.ndarray, B_list):\n",
    "    \"\"\"\n",
    "    Compute Δ(π,B) = max_n [ J_{c_n,U}(π) − B[n] ] exactly using\n",
    "    dynamic programming.  This helper is used in the outer loop to\n",
    "    determine the sign for PBA.\n",
    "\n",
    "    Args:\n",
    "        pi_flat (np.ndarray): current policy (flattened)\n",
    "        B_list (list or np.ndarray): threshold list\n",
    "    Returns:\n",
    "        float: maximum violation across costs\n",
    "    \"\"\"\n",
    "    # Construct Q_initial_list of zeros for all costs\n",
    "    Q_initial_list = [np.zeros((S, A)) for _ in range(N)]\n",
    "    J_results, _, _ = compute_J_c_U_b_and_its_max(U, C, Q_initial_list, B_list, pi_flat)\n",
    "    return float(max(J_results))\n",
    "\n",
    "\n",
    "# ------------------------- Sampling utilities -------------------------\n",
    "\n",
    "def sample_trajectory(pi_flat: np.ndarray,\n",
    "                      P_kernel: np.ndarray,\n",
    "                      horizon: int = H) -> tuple[list[int], list[int]]:\n",
    "    \"\"\"\n",
    "    Sample a single trajectory of states and actions under a given\n",
    "    policy and transition kernel.\n",
    "\n",
    "    Args:\n",
    "        pi_flat (np.ndarray): flattened policy of length S*A\n",
    "        P_kernel (np.ndarray): transition kernel of shape (S,A,S)\n",
    "        horizon (int): number of steps to simulate\n",
    "    Returns:\n",
    "        tuple: (states, actions) lists of length ``horizon``\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    actions = []\n",
    "    pi_matrix = pi_flat.reshape(S, A)\n",
    "    # Sample initial state according to μ\n",
    "    current_state = np.random.choice(S, p=mu)\n",
    "    for _ in range(horizon):\n",
    "        # Sample action according to current policy for this state\n",
    "        action = np.random.choice(A, p=pi_matrix[current_state])\n",
    "        states.append(current_state)\n",
    "        actions.append(action)\n",
    "        # Sample next state according to P_kernel\n",
    "        next_state = np.random.choice(S, p=P_kernel[current_state, action, :])\n",
    "        current_state = next_state\n",
    "    return states, actions\n",
    "\n",
    "\n",
    "def sample_return_for_cost(pi_flat: np.ndarray,\n",
    "                           P_kernel: np.ndarray,\n",
    "                           cost_sa: np.ndarray,\n",
    "                           horizon: int = H) -> float:\n",
    "    \"\"\"\n",
    "    Estimate J_{c,P}(π) by sampling a single trajectory and summing\n",
    "    discounted costs.\n",
    "\n",
    "    Args:\n",
    "        pi_flat (np.ndarray): flattened policy\n",
    "        P_kernel (np.ndarray): transition kernel\n",
    "        cost_sa (np.ndarray): cost matrix of shape (S,A)\n",
    "        horizon (int): number of steps to simulate\n",
    "    Returns:\n",
    "        float: sampled discounted return\n",
    "    \"\"\"\n",
    "    states, actions = sample_trajectory(pi_flat, P_kernel, horizon)\n",
    "    ret = 0.0\n",
    "    discount = 1.0\n",
    "    for t, (s, a) in enumerate(zip(states, actions)):\n",
    "        ret += discount * cost_sa[s, a]\n",
    "        discount *= gamma\n",
    "    return ret\n",
    "\n",
    "\n",
    "def approximate_J_c_P(pi_flat: np.ndarray,\n",
    "                      P_kernel: np.ndarray,\n",
    "                      cost_sa: np.ndarray,\n",
    "                      num_samples: int = 1,\n",
    "                      horizon: int = H) -> float:\n",
    "    \"\"\"\n",
    "    Approximate the expected discounted cost J_{c,P}(π) via Monte Carlo\n",
    "    sampling.\n",
    "\n",
    "    Args:\n",
    "        pi_flat (np.ndarray): flattened policy\n",
    "        P_kernel (np.ndarray): transition kernel\n",
    "        cost_sa (np.ndarray): cost matrix\n",
    "        num_samples (int): number of trajectories to sample\n",
    "        horizon (int): horizon length\n",
    "    Returns:\n",
    "        float: Monte Carlo estimate of the expected discounted return\n",
    "    \"\"\"\n",
    "    total = 0.0\n",
    "    for _ in range(num_samples):\n",
    "        total += sample_return_for_cost(pi_flat, P_kernel, cost_sa, horizon)\n",
    "    return total / float(num_samples)\n",
    "\n",
    "\n",
    "def approximate_J_c_U(pi_flat: np.ndarray,\n",
    "                      C_list: list,\n",
    "                      U_list: list,\n",
    "                      num_samples: int = 1,\n",
    "                      horizon: int = H) -> tuple[list[float], list[int]]:\n",
    "    \"\"\"\n",
    "    Approximate J_{c_n,U}(π) for each cost n by sampling.  For each\n",
    "    cost the worst case over the uncertainty set is returned.\n",
    "\n",
    "    Args:\n",
    "        pi_flat (np.ndarray): current policy\n",
    "        C_list (list): list of cost matrices c_n\n",
    "        U_list (list): list of transition kernels\n",
    "        num_samples (int): number of trajectories per environment\n",
    "        horizon (int): horizon length\n",
    "    Returns:\n",
    "        tuple: (J_estimates, worst_env_indices)\n",
    "            J_estimates[n] ≈ max_i J_{c_n,U_i}(π)\n",
    "            worst_env_indices[n] = argmax_i J_{c_n,U_i}(π)\n",
    "    \"\"\"\n",
    "    J_estimates = []\n",
    "    worst_env_indices = []\n",
    "    for n in range(N):\n",
    "        values = []\n",
    "        for i in range(U_num):\n",
    "            val = approximate_J_c_P(pi_flat, U_list[i], C_list[n], num_samples, horizon)\n",
    "            values.append(val)\n",
    "        idx = int(np.argmax(values))\n",
    "        J_estimates.append(values[idx])\n",
    "        worst_env_indices.append(idx)\n",
    "    return J_estimates, worst_env_indices\n",
    "\n",
    "\n",
    "def approximate_delta(pi_flat: np.ndarray,\n",
    "                      B_list,\n",
    "                      C_list: list,\n",
    "                      U_list: list,\n",
    "                      num_samples: int = 1,\n",
    "                      horizon: int = H) -> float:\n",
    "    \"\"\"\n",
    "    Compute an approximate maximum constraint violation Δ for a\n",
    "    candidate policy using Monte Carlo sampling.\n",
    "\n",
    "    Args:\n",
    "        pi_flat (np.ndarray): current policy\n",
    "        B_list (list): thresholds b_n\n",
    "        C_list (list): list of cost matrices c_n\n",
    "        U_list (list): list of transition kernels\n",
    "        num_samples (int): number of samples per environment\n",
    "        horizon (int): horizon length\n",
    "    Returns:\n",
    "        float: estimated Δ\n",
    "    \"\"\"\n",
    "    J_estimates, _ = approximate_J_c_U(pi_flat, C_list, U_list, num_samples, horizon)\n",
    "    max_violation = max([J_estimates[n] - B_list[n] for n in range(N)])\n",
    "    return float(max_violation)\n",
    "\n",
    "\n",
    "def estimate_gradient_REINFORCE(pi_flat: np.ndarray,\n",
    "                                P_kernel: np.ndarray,\n",
    "                                cost_sa: np.ndarray,\n",
    "                                num_samples: int = 1,\n",
    "                                horizon: int = H) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Estimate the gradient of J_{c,P}(π) with respect to π using the\n",
    "    REINFORCE estimator.\n",
    "\n",
    "    This function returns a gradient table of shape (S,A).  For each\n",
    "    sampled trajectory it computes the cumulative discounted cost from\n",
    "    each time step and multiplies it by the derivative of the log\n",
    "    policy with respect to the action probability.  The resulting\n",
    "    gradient is averaged over ``num_samples`` trajectories.\n",
    "\n",
    "    Args:\n",
    "        pi_flat (np.ndarray): flattened policy\n",
    "        P_kernel (np.ndarray): transition kernel\n",
    "        cost_sa (np.ndarray): cost matrix\n",
    "        num_samples (int): number of trajectories to sample\n",
    "        horizon (int): horizon length\n",
    "    Returns:\n",
    "        np.ndarray: gradient estimate of shape (S,A)\n",
    "    \"\"\"\n",
    "    gradient = np.zeros((S, A))\n",
    "    pi_matrix = pi_flat.reshape(S, A)\n",
    "    for _ in range(num_samples):\n",
    "        states, actions = sample_trajectory(pi_flat, P_kernel, horizon)\n",
    "        # Compute discounted return from each time step\n",
    "        # returns_from_t[t] = sum_{k=t}^{T-1} gamma^{k-t} c(s_k,a_k)\n",
    "        G_remaining = 0.0\n",
    "        returns_from_t = np.zeros(len(states))\n",
    "        for idx in reversed(range(len(states))):\n",
    "            s = states[idx]\n",
    "            a = actions[idx]\n",
    "            G_remaining = cost_sa[s, a] + gamma * G_remaining\n",
    "            returns_from_t[idx] = G_remaining\n",
    "        # Accumulate gradient using score function estimator\n",
    "        for idx, (s, a) in enumerate(zip(states, actions)):\n",
    "            # derivative of log π(a|s) w.r.t. π(s,a) is 1/π(s,a)\n",
    "            gradient[s, a] += returns_from_t[idx] / (pi_matrix[s, a] + 1e-8)\n",
    "    gradient /= float(num_samples)\n",
    "    return gradient\n",
    "\n",
    "\n",
    "# ------------------------- Inner and outer loops -------------------------\n",
    "\n",
    "# The following global containers are used to record learning\n",
    "# statistics.  When running ``pba_outer_loop`` these lists are\n",
    "# populated to enable downstream visualisation.  If you run multiple\n",
    "# experiments in a single interpreter session you may wish to reset\n",
    "# them beforehand.\n",
    "outer_b0_history: list[float] = []\n",
    "outer_delta_history: list[float] = []\n",
    "outer_sign_history: list[int] = []\n",
    "inner_deltas_history: list[list[float]] = []\n",
    "\n",
    "def inner_loop(b0: float,\n",
    "               initial_pi_flat: np.ndarray,\n",
    "               record_list: list[float] | None = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Execute the stochastic inner loop (Algorithm 2) for a fixed\n",
    "    threshold ``b0``.\n",
    "\n",
    "    Given a candidate objective threshold ``b0`` this routine seeks a\n",
    "    policy π that minimises the maximum constraint violation Δ.  The\n",
    "    objective threshold ``B[0]`` is set to ``b0`` temporarily.  A\n",
    "    REINFORCE gradient step is performed at each iteration using the\n",
    "    worst case cost component and environment determined from\n",
    "    approximate value estimates.  The best policy encountered (in\n",
    "    terms of Δ) during the iterations is returned.\n",
    "\n",
    "    Args:\n",
    "        b0 (float): objective threshold\n",
    "        initial_pi_flat (np.ndarray): starting policy vector\n",
    "    Returns:\n",
    "        np.ndarray: flattened policy found by the inner loop\n",
    "    \"\"\"\n",
    "    # Update the objective threshold for the duration of this call\n",
    "    B_local = B.copy()\n",
    "    B_local[0] = b0\n",
    "    # Local copy of the policy to avoid clobbering the global state\n",
    "    local_pi = initial_pi_flat.copy()\n",
    "    best_pi = local_pi.copy()\n",
    "    # Compute the current best Δ value\n",
    "    best_delta_val = approximate_delta(local_pi, B_local, C, U)\n",
    "    for t in range(T_inner):\n",
    "        # Estimate worst case costs and environments\n",
    "        J_estimates, worst_env_indices = approximate_J_c_U(local_pi, C, U)\n",
    "        # Determine the cost index with maximum violation\n",
    "        delta_values = [J_estimates[n] - B_local[n] for n in range(N)]\n",
    "        worst_cost_idx = int(np.argmax(delta_values))\n",
    "        worst_env_idx = worst_env_indices[worst_cost_idx]\n",
    "        # Estimate gradient for this worst case\n",
    "        grad = estimate_gradient_REINFORCE(local_pi,\n",
    "                                           U[worst_env_idx],\n",
    "                                           C[worst_cost_idx],\n",
    "                                           num_samples=1,\n",
    "                                           horizon=H)\n",
    "        # Gradient descent update and projection\n",
    "        updated_pi_matrix = update_and_project_policy(\n",
    "            local_pi.reshape(S, A), grad, alpha)\n",
    "        local_pi = updated_pi_matrix.flatten()\n",
    "        # Track the best policy seen so far according to Δ\n",
    "        current_delta = approximate_delta(local_pi, B_local, C, U)\n",
    "        if current_delta < best_delta_val:\n",
    "            best_delta_val = current_delta\n",
    "            best_pi = local_pi.copy()\n",
    "        # Record approximate Δ if logging is requested\n",
    "        if record_list is not None:\n",
    "            record_list.append(current_delta)\n",
    "        # Optional diagnostics: uncomment for verbose output\n",
    "        # if t % 10 == 0:\n",
    "        #     print(f\"  Inner iter {t}: Δ ≈ {current_delta:.4f}, b0 = {b0:.4f}\")\n",
    "    return best_pi\n",
    "\n",
    "\n",
    "def pba_outer_loop(initial_pi_flat: np.ndarray, return_logs: bool = False) -> np.ndarray | tuple[np.ndarray, dict]:\n",
    "    \"\"\"\n",
    "    Execute the probabilistic bisection outer loop (Algorithm 1).\n",
    "\n",
    "    This routine implements a discrete version of the PBA described in\n",
    "    the paper.  A belief distribution over the interval [0, H] is\n",
    "    maintained on a finite grid.  At each iteration the median of the\n",
    "    belief determines the test threshold b0.  The inner loop is\n",
    "    executed with this threshold and the sign of the resulting Δ\n",
    "    updates the belief via Bayes’ rule using the reliability\n",
    "    parameter ``p_reliability``.  After ``K_outer`` iterations the\n",
    "    median of the final belief yields the estimate j(K), and a final\n",
    "    call to the inner loop returns the policy at that threshold.\n",
    "\n",
    "    Args:\n",
    "        initial_pi_flat (np.ndarray): initial policy vector\n",
    "    Returns:\n",
    "        np.ndarray: flattened policy returned by S‑EpiRC‑PGS\n",
    "    \"\"\"\n",
    "    # Initialize belief over [0, H] as uniform discrete distribution\n",
    "    grid = np.linspace(0.0, float(H), belief_grid_size)\n",
    "    belief = np.ones(belief_grid_size)\n",
    "    belief /= belief.sum()\n",
    "    # Current policy to be refined by successive inner loop calls\n",
    "    current_pi = initial_pi_flat.copy()\n",
    "    for k in range(K_outer):\n",
    "        # Compute the CDF and find the median index\n",
    "        cdf = np.cumsum(belief)\n",
    "        median_idx = int(np.searchsorted(cdf, 0.5))\n",
    "        b0 = grid[median_idx]\n",
    "        # Prepare a container to record Δ during the inner iterations\n",
    "        inner_deltas: list[float] = []\n",
    "        # Run inner loop to (approximately) solve the auxiliary problem\n",
    "        pi_k = inner_loop(b0, current_pi, record_list=inner_deltas)\n",
    "        # Estimate Δ exactly using dynamic programming for robustness\n",
    "        delta_val = compute_delta(pi_k, [b0] + B[1:])\n",
    "        # Determine sign: +1 if Δ > 0 (b0 too small) else -1\n",
    "        Z_k = 1 if delta_val > 0 else -1\n",
    "        # Record outer loop statistics\n",
    "        outer_b0_history.append(float(b0))\n",
    "        outer_delta_history.append(float(delta_val))\n",
    "        outer_sign_history.append(int(Z_k))\n",
    "        inner_deltas_history.append(inner_deltas)\n",
    "\n",
    "        # Update belief according to PBA update rule\n",
    "        # If Z_k = +1, true J⋆ > b0, so upweight the right half\n",
    "        if Z_k == 1:\n",
    "            mask = grid <= b0\n",
    "            belief[mask] *= (1.0 - p_reliability)\n",
    "            belief[~mask] *= p_reliability\n",
    "        else:\n",
    "            # Z_k == -1 implies J⋆ ≤ b0, upweight the left half\n",
    "            mask = grid <= b0\n",
    "            belief[mask] *= p_reliability\n",
    "            belief[~mask] *= (1.0 - p_reliability)\n",
    "        # Normalise the belief distribution\n",
    "        belief_sum = belief.sum()\n",
    "        if belief_sum > 0:\n",
    "            belief /= belief_sum\n",
    "        else:\n",
    "            # Avoid degenerate case: reinitialise to uniform\n",
    "            belief = np.ones_like(belief) / belief.size\n",
    "        # Update current policy for next iteration\n",
    "        current_pi = pi_k.copy()\n",
    "        # Optional diagnostics: uncomment for verbose output\n",
    "        # print(f\"Outer iter {k}: b0 = {b0:.4f}, Δ = {delta_val:.4f}, sign = {Z_k:+d}\")\n",
    "    # Final threshold is the median of the final belief\n",
    "    cdf_final = np.cumsum(belief)\n",
    "    median_idx_final = int(np.searchsorted(cdf_final, 0.5))\n",
    "    b_final = grid[median_idx_final]\n",
    "    # Final inner loop call to get the returned policy\n",
    "    final_inner_deltas: list[float] = []\n",
    "    final_pi = inner_loop(b_final, current_pi, record_list=final_inner_deltas)\n",
    "    # Record the final inner deltas but do not append b_final/delta/sign again\n",
    "    inner_deltas_history.append(final_inner_deltas)\n",
    "    if return_logs:\n",
    "        # Package the logged data into a dictionary\n",
    "        logs = {\n",
    "            \"outer_b0\": outer_b0_history,\n",
    "            \"outer_delta\": outer_delta_history,\n",
    "            \"outer_sign\": outer_sign_history,\n",
    "            \"inner_deltas\": inner_deltas_history,\n",
    "            \"final_b\": float(b_final)\n",
    "        }\n",
    "        return final_pi, logs\n",
    "    else:\n",
    "        return final_pi\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the algorithm and collect logs for visualisation\n",
    "    final_policy, logs = pba_outer_loop(pi_flat, return_logs=True)\n",
    "    print(\"Final policy (flattened) from S‑EpiRC‑PGS:\")\n",
    "    print(final_policy)\n",
    "    # Generate visualisations of the learning process and final policy\n",
    "    try:\n",
    "        import matplotlib\n",
    "        matplotlib.use('Agg')  # Use a non‑interactive backend\n",
    "        import matplotlib.pyplot as plt\n",
    "        # Outer loop metrics\n",
    "        outer_iters = range(len(logs[\"outer_b0\"]))\n",
    "        fig, ax = plt.subplots(2, 1, figsize=(8, 8), constrained_layout=True)\n",
    "        ax[0].plot(outer_iters, logs[\"outer_b0\"], marker='o', label='Threshold b0')\n",
    "        ax[0].plot(outer_iters, logs[\"outer_delta\"], marker='x', label='Δ (violation)')\n",
    "        # Use scatter to indicate sign of Δ (positive/negative)\n",
    "        colors = ['tab:red' if s > 0 else 'tab:green' for s in logs[\"outer_sign\"]]\n",
    "        ax[0].scatter(outer_iters, logs[\"outer_delta\"], c=colors, label='Sign of Δ')\n",
    "        ax[0].set_xlabel('Outer iteration')\n",
    "        ax[0].set_ylabel('Value')\n",
    "        ax[0].set_title('Outer loop evolution')\n",
    "        ax[0].legend()\n",
    "        # Inner loop curves: plot Δ per inner iteration for each outer loop\n",
    "        for idx, inner_deltas in enumerate(logs[\"inner_deltas\"][:-1]):\n",
    "            ax[1].plot(range(len(inner_deltas)), inner_deltas, label=f'outer {idx}')\n",
    "        ax[1].set_xlabel('Inner iteration')\n",
    "        ax[1].set_ylabel('Δ')\n",
    "        ax[1].set_title('Inner loop Δ over iterations')\n",
    "        if len(logs[\"inner_deltas\"]) > 1:\n",
    "            ax[1].legend(loc='upper right', fontsize='small', ncol=2)\n",
    "        # Save the figure summarising the learning process\n",
    "        fig.savefig('learning_results.png')\n",
    "        plt.close(fig)\n",
    "        # Visualise final policy as both a heatmap and bar chart\n",
    "        policy_matrix = final_policy.reshape(S, A)\n",
    "        # Heatmap\n",
    "        fig2, ax2 = plt.subplots(figsize=(4, 3))\n",
    "        im = ax2.imshow(policy_matrix, cmap='viridis', aspect='auto')\n",
    "        ax2.set_xticks(range(A))\n",
    "        ax2.set_yticks(range(S))\n",
    "        ax2.set_xlabel('Action')\n",
    "        ax2.set_ylabel('State')\n",
    "        ax2.set_title('Final policy heatmap')\n",
    "        for i in range(S):\n",
    "            for j in range(A):\n",
    "                val = policy_matrix[i, j]\n",
    "                ax2.text(j, i, f\"{val:.2f}\", ha='center', va='center', color='white' if val > 0.5 else 'black')\n",
    "        fig2.colorbar(im, ax=ax2, fraction=0.046, pad=0.04)\n",
    "        fig2.savefig('final_policy_heatmap.png')\n",
    "        plt.close(fig2)\n",
    "        # Bar chart for each state\n",
    "        fig3, ax3 = plt.subplots(figsize=(6, 4))\n",
    "        x = np.arange(A)\n",
    "        width = 0.8 / S\n",
    "        for s_idx in range(S):\n",
    "            ax3.bar(x + s_idx * width, policy_matrix[s_idx], width, label=f'state {s_idx}')\n",
    "        ax3.set_xticks(x + width * (S - 1) / 2)\n",
    "        ax3.set_xticklabels([f'action {a}' for a in range(A)])\n",
    "        ax3.set_ylabel('Probability')\n",
    "        ax3.set_title('Final policy bar chart')\n",
    "        ax3.legend()\n",
    "        fig3.savefig('final_policy_barchart.png')\n",
    "        plt.close(fig3)\n",
    "        print(\"Learning curves and final policy visualisations have been saved as PNG files.\")\n",
    "    except Exception as e:\n",
    "        print(\"Matplotlib is not available or an error occurred while plotting:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
