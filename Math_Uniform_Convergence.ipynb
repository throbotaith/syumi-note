{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b86b56",
   "metadata": {},
   "source": [
    "# このページについて\n",
    "一様収束と汎化解析にかんする勉強のまとめです．作成に当たっては、トン・ジャン先生の[機械学習のための数理解析](https://tongzhang-ml.org/)を大いに参考にさせていただきました。先生、本当にありがとうございます．\n",
    "\n",
    "## Probably Approximately Correct Learning(PAC)\n",
    "\n",
    "### 準備\n",
    "\n",
    "PAC学習とは，ブール関数つまり分類関数を学習するアルゴリズムを解析するためのモデルです．\n",
    "\n",
    "**設定**\n",
    "* 未知の分布：$\\mathcal{D}$\n",
    "* 入力: $d$個の0か1からなるベクトル $X \\in \\{0,1\\}^d$ \n",
    "* 真の関数:  $f_*: \\{0,1\\}^d \\to \\{0,1\\}$ \n",
    "\t*  $X$ を受け取って、0か1の出力を返します。conceptとも\n",
    "* 概念クラス(関数の集合):  $\\mathcal{C}$ \n",
    "* オラクル$\\mathcal{O}$：$\\mathcal{D}$ からサンプルされた$X$と，それを真の関数に入力した場合の出力$f_*(X)$ のペア$(X, Y)$を出力 \n",
    "* 学習アルゴリズム：$\\mathcal{A}$\n",
    "* $\\mathcal{O}$の呼び出し回数：$n$\n",
    "* 訓練データ：$\\mathcal{S}_n = \\{(X_i, Y_i)\\}_{i=1}^n$\n",
    "\n",
    "**学習方法**\n",
    "* $\\mathcal{A}$ は$\\mathcal{O}$を $n$ 回呼び出して $\\mathcal{S}_n = \\{(X_i, Y_i)\\}_{i=1}^n$ をサンプルします．\n",
    "* $\\mathcal{A}$ は、このデータをもとに、 $\\mathcal{C}$ の中から一つの関数 $\\hat{f}$ を選び出して出力します．\n",
    "\n",
    "**学習評価**\n",
    " $\\hat{f}$ は、汎化誤差で評価されます。\n",
    "$$\n",
    "\\operatorname{err}_{\\mathcal{D}}(f) = \\mathbb{E}_{X \\sim \\mathcal{D}} \\mathbb{1}(f(x) \\neq f_*(x)) \n",
    "$$\n",
    "期待値展開をすると以下のようになります．\n",
    "$$\n",
    "\\mathbb{E}_{X \\sim \\mathcal{D}} \\mathbb{1}(f(x) \\neq f_*(x)) = \\sum_{x \\in \\{0,1\\}^d} P(X=x) \\cdot \\mathbb{1}(f(x) \\neq f_*(x))\n",
    "$$\n",
    "支持関数的なのが式内に入っており，実質的にはこの式は以下のように変形できます．\n",
    "$$\n",
    "\\sum_{x \\in \\{0,1\\}^d} P(X=x) \\cdot \\mathbb{1}(f(x) \\neq f_*(x)) = \\sum_{x \\text{ s.t. } f(x) \\neq f_*(x)} P(X=x)\n",
    "$$\n",
    "よって，\n",
    "$$\n",
    "\\operatorname{err}_{\\mathcal{D}}(f) = \\operatorname{Pr}_{X \\sim \\mathcal{D}}(f(X) \\neq f_*(X))\n",
    "$$\n",
    "です．つまり汎化誤差とは，$f$の予測が，$f_*$の値と異なる確率を示しており，この確率を最小化すれば，汎化できているというわけです．\n",
    "\n",
    "### PAC learnable(定義3.3)\n",
    "ある $\\mathcal{A}$ が存在し、すべての $f_{*} \\in \\mathcal{C}$、 $\\mathcal{D}$、 $\\epsilon>0$、 $\\delta \\in(0,1)$ に対して、**以下の命題**が成り立つ場合、 $\\mathcal{C}$ は**PAC learnable**です．\n",
    "\n",
    "**命題**\n",
    "$\\mathcal{O}$ から $\\mathcal{D}$ 上のサンプルに対して少なくとも $1-\\delta$ の確率で、学習器は次を満たす関数 $\\hat{f}$ を生成する．\n",
    "$$\n",
    "\\operatorname{err}_{\\mathcal{D}}(\\hat{f}) \\leq \\epsilon\n",
    "$$\n",
    "言い換えると，\n",
    "$$\n",
    "\\Pr(\\operatorname{err}_{\\mathcal{D}}(\\hat{f}) \\leq \\epsilon) \\geq 1-\\delta\n",
    "$$\n",
    "$$\n",
    "\\Pr(\\operatorname{Pr}_{X \\sim \\mathcal{D}}(f(X) \\neq f_*(X)) \\leq \\epsilon) \\geq 1-\\delta\n",
    "$$\n",
    "が成立するという命題です．つまり，$f$の予測が，$f_*$の値と異なる確率が$\\epsilon$より，低くなるとき，それが発生する確率は少なくとも $1-\\delta$ であるということです．\n",
    "\n",
    "### サンプル複雑度\n",
    "$\\operatorname{err}_{\\mathcal{D}}(\\hat{f}) \\leq \\epsilon$を満たすような関数を獲得するために必要なオラクルのサンプル数の下界をサンプル複雑度と言いいます．\n",
    "例えば，$\\operatorname{err}_{\\mathcal{D}}(\\hat{f}) \\leq\\sqrt{\\frac{\\ln(N/\\delta)}{2n}}$だとします．右辺が，$\\epsilon$以下になるようなサンプル数を求めたいわけです．つまり，以下のように式変形をして下界を導出します．\n",
    "\n",
    "$$ \\sqrt{\\frac{\\ln(N/\\delta)}{2n}} \\leq \\epsilon $$\n",
    "$$ \\frac{\\ln(N/\\delta)}{2n} \\leq \\epsilon^2 $$\n",
    "$n$ について解くと，\n",
    "$$ \\frac{\\ln(N/\\delta)}{2\\epsilon^2} \\leq n $$\n",
    "最終的に、\n",
    "$$ n \\geq \\frac{\\ln(N/\\delta)}{2\\epsilon^2} = \\frac{\\ln N + \\ln(1/\\delta)}{2\\epsilon^2} $$\n",
    "\n",
    "この結果から、この場合のサンプル複雑度は $n = O\\left(\\frac{\\ln N + \\ln(1/\\delta)}{\\epsilon^2}\\right)$ となります．\n",
    "\n",
    "**Empirical Risk Minimization(定義3.4)**\n",
    " $f \\in \\mathcal{C}$ のTraining Errorを次のように定義します．\n",
    "$$ \\widehat{\\operatorname{err}}_{\\mathcal{S}_{n}}(f) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{1}(f(X_i) \\neq Y_i) $$\n",
    "\n",
    "### ユニオンバウンド(命題 3.5)\n",
    " $m$ 個の事象 $E_1, \\dots, E_m$ に対して、以下の確率不等式が成り立ちます．よく使います．\n",
    "$$ \\operatorname{Pr}(E_1 \\cup \\dots \\cup E_m) \\leq \\sum_{j=1}^m \\operatorname{Pr}(E_j) $$\n",
    "\n",
    "**有限仮説空間におけるERMのPACバウンド(定理3.6)**\n",
    " $N$ 個の要素を持つ $\\mathcal{C}$ ,ある $\\gamma > 0$ に対して $\\epsilon' = \\gamma^2 \\frac{2 \\ln(N/\\delta)}{n}$ を満たす近似ERMアルゴリズムが出力する $\\hat{f}$ は、\n",
    "少なくとも確率 $1-\\delta$ で、以下の汎化誤差バウンドを満たします\n",
    "$$ \\operatorname{err}_{\\mathcal{D}}(\\hat{f}) \\leq (1+\\gamma)^2 \\frac{2 \\ln(N/\\delta)}{n} $$\n",
    "\n",
    "**損失関数の単純化表記(定義 3.8)**\n",
    "観測 $Z=(X,Y)$、パラメータ $w \\in \\Omega$、予測関数 $f(w, X)$、損失関数 $L(f(w, X), Y)$ \n",
    "*   **差分損失関数 ($\\phi(w, z)$):**\n",
    "$$ \\phi(w, z) = L(f(w, x), y) - L_*(x, y) $$\n",
    "*   **訓練損失 ($\\phi(w, \\mathcal{S}_n)$):** \n",
    "$$ \\phi(w, \\mathcal{S}_n) = \\frac{1}{n} \\sum_{i=1}^n \\phi(w, Z_i) $$\n",
    "*   **テスト損失 ($\\phi(w, \\mathcal{D})$):** \n",
    "$$ \\phi(w, \\mathcal{D}) = \\mathbb{E}_{Z \\sim \\mathcal{D}} \\phi(w, Z) $$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
