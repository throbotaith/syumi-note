{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ロバストマルコフ決定過程の基礎\n",
    "\n",
    "\n",
    "MDPにおける遷移関数モデルや報酬関数モデルのパラメータが正確に把握できない状況により、各モデルに不確実性が生じる場合を想定します。RMDPでは、モデルの不確実性を考慮しながら最適方策を求めるアプローチの1つです。RMDPは、真の遷移関数が不確実性集合という遷移関数の集合の中に属すると仮定し、不確実性集合内の最悪ケース(総報酬が低くなるor総コストが高くなる)遷移関数において、報酬が高くなるor総コストが低くなる方策(ロバスト方策)を求めます。\n",
    "\n",
    "\n",
    "\n",
    "## 不確実性集合\n",
    "不確実性集合に仮定を置かない一般的なRMDPを解くことはNP困難になる可能性があります(Wiesemann et al., 2013)[75]。これに対し、不確実性集合に対して$(s,a)or(s)$-rectangular、という仮定を置くことを考えます。仮定により、動的計画法を用いてRMDPを解くことが可能となり、near-optimalなロバスト方策を獲得することができます。\n",
    "\n",
    "### 仮定：$(s,a),(s)$-rectangular\n",
    "\n",
    "**$(s,a)$-rectangular**\n",
    "不確実性集合を$\\mathcal{P}$と仮定します。以下を満たす場合、$\\mathcal{P}$は$(s,a)$-rectangular setsと呼ばれます。$X$は直積を示しています。$\\mathcal{P}_{s, a} \\subseteq \\Delta(\\mathcal{S})$(状態に対する確率単体の部分集合)\n",
    "\n",
    "$$\n",
    "\\mathcal{P}=\\underset{(s, a) \\in \\mathcal{S} \\times \\mathcal{A}}{X} \\mathcal{P}_{s, a}\n",
    "$$\n",
    "**解説と例**\n",
    "$\\mathcal{P}_{s, a}$は、$s,a \\in \\mathcal{S},{A}$を入力した場合に、$s\\in S$の確率を出力する関数の集合ととらえることができます。$\\underset{(s, a) \\in \\mathcal{S} \\times \\mathcal{A}}{X} \\mathcal{P}_{s, a}$は任意の$s,a$に対して$s\\in S$の確率を出力する関数の集合が存在するということを意味しています。\n",
    "$|S|=2,|A|=2$を考えます。$\\mathcal{P}_{1, 1},\\mathcal{P}_{1, 2},\\mathcal{P}_{2, 1},\\mathcal{P}_{2, 2} \\subseteq \\Delta(\\mathcal{S})$とします。\n",
    "- $\\mathcal{P}_{1,1} = {(P(1|1,1)=0.1, P(2|1,1)=0.9), (P(1|1,1)=0.7, P(2|1,1)=0.3)}$\n",
    "- $\\mathcal{P}_{1,2} = {(P(1|1,2)=0.4, P(2|1,2)=0.6), (P(1|1,2)=0.8, P(2|1,2)=0.2)}$\n",
    "- $\\mathcal{P}_{2,1} = {(P(1|2,1)=0.5, P(2|2,1)=0.5), (P(1|2,1)=0.6, P(2|2,1)=0.4)}$\n",
    "- $\\mathcal{P}_{2,2} = {(P(1|2,2)=0.3, P(2|2,2)=0.7), (P(1|2,2)=0.2, P(2|2,2)=0.8)}$\n",
    "のようにあらわされます。\n",
    "\n",
    "**$(s)$-rectangular**\n",
    "$\\mathcal{P}_{s} \\subseteq \\Delta(\\mathcal{S})^{|\\mathcal{A}|}$ であり、$\\Delta(\\mathcal{S})^{|\\mathcal{A}|}:=\\left\\{\\left(P_{a}\\right)_{a \\in \\mathcal{A}} \\mid P_{a} \\in \\Delta(\\mathcal{S})\\right.$, for all $\\left.a \\in \\mathcal{A}\\right\\}$\n",
    "とします。同様に、以下を満たす場合、$\\mathcal{P}$は$(s)$-rectangular setsと呼ばれます。\n",
    "$$\n",
    "\\mathcal{P}=\\underset{s \\in \\mathcal{S}}{X} \\mathcal{P}_{s}\n",
    "$$\n",
    "**解説と例**\n",
    "$\\mathcal{P}_{s}$は任意の$s$を入力とし、その$s$のうえで実行できる$a$について、その遷移確率すべてを要素とした集合の部分集合といえます。各 $\\mathcal{P}_{s} \\subseteq \\Delta(\\mathcal{S})^{|\\mathcal{A}|}$ は次のように表されます：\n",
    "- $\\mathcal{P}_{1} = {((P(1|1,1)=0.1, P(2|1,1)=0.9), (P(1|1,2)=0.4, P(2|1,2)=0.6)),\\\\((P(1|1,1)=0.7, P(2|1,1)=0.3), (P(1|1,2)=0.8, P(2|1,2)=0.2))}$\n",
    "- $\\mathcal{P}_{2} = {((P(1|2,1)=0.5, P(2|2,1)=0.5), (P(1|2,2)=0.3, P(2|2,2)=0.7)),\\ ((P(1|2,1)=0.6, P(2|2,1)=0.4), (P(1|2,2)=0.2, P(2|2,2)=0.8))}$\n",
    "\n",
    "### 不確実性集合の例\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### (コラム)なぜ不確実性集合に仮定を置かないとNP困難なのか\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMDPを解く(関連研究)\n",
    "\n",
    "\n",
    "### ロバスト動的計画法\n",
    "(Iyengar, 2005; Nilim & El Ghaoui, 2005; Kaufman & Schaefer, 2013; Ho et al., 2021)\n",
    "\n",
    "#### 準備\n",
    "Iyengarの研究を取り上げます。最初に、割引無限マルコフ決定過程を定義し,いくつかの関数を用意します。(kitamuraさん)\n",
    "1. 有限状態集合: $S=\\{1, \\dots, |S|\\}$\n",
    "2. 有限行動集合: $A=\\{1, \\dots, |A|\\}$\n",
    "3. 遷移確率行列: $P\\in \\mathbb{R}^{SA\\times S}$\n",
    "4. 報酬行列: $r\\in \\mathbb{R}^{S\\times A}$\n",
    "5. 割引率: $\\gamma \\in [0, 1)$\n",
    "6. 初期状態: $\\mu \\in \\mathbb{R}^{S}$\n",
    "\n",
    "* ``compute_greedy_policy``: Q関数 ($H\\times S \\times A \\to \\mathcal{R}$) の貪欲方策を返します\n",
    "* ``compute_optimal_Q``: MDPの最適Q関数 $q_* : H\\times S \\times A \\to \\mathcal{R}$ を返します。\n",
    "* ``compute_policy_Q``: 方策 $\\pi$ のQ関数 $q_\\pi : H\\times S \\times A \\to \\mathcal{R}$ を返します。\n",
    "* ``sample_next_state``: ステップ・状態・行動の集合$D$のそれぞれについて次状態を$N$個返します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状態数： 10\n",
      "行動数： 3\n",
      "割引率： 0.8\n",
      "ホライゾン： 5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax.random import PRNGKey\n",
    "import jax\n",
    "from typing import NamedTuple, Optional\n",
    "from functools import partial\n",
    "import chex\n",
    "\n",
    "key = PRNGKey(0)\n",
    "\n",
    "S = 10  # 状態集合のサイズ\n",
    "A = 3  # 行動集合のサイズ\n",
    "S_set = jnp.arange(S)  # 状態集合\n",
    "A_set = jnp.arange(A)  # 行動集合\n",
    "gamma = 0.8  # 割引率\n",
    "\n",
    "\n",
    "# 報酬行列を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "rew = jax.random.uniform(key=key, shape=(S, A))\n",
    "assert rew.shape == (S, A)\n",
    "\n",
    "\n",
    "# 遷移確率行列を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "P = jax.random.uniform(key=key, shape=(S*A, S))\n",
    "P = P / jnp.sum(P, axis=-1, keepdims=True)  # 正規化して確率にします\n",
    "P = P.reshape(S, A, S)\n",
    "np.testing.assert_allclose(P.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "\n",
    "# 初期状態分布を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "mu = jax.random.uniform(key, shape=(S,))\n",
    "mu = mu / jnp.sum(mu)\n",
    "np.testing.assert_allclose(mu.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "\n",
    "# 状態集合, 行動集合, 割引率, 報酬行列, 遷移確率行列が準備できたのでMDPのクラスを作ります\n",
    "\n",
    "class MDP(NamedTuple):\n",
    "    S_set: jnp.array  # 状態集合\n",
    "    A_set: jnp.array  # 行動集合\n",
    "    gamma: float  # 割引率\n",
    "    H: int  # エフェクティブホライゾン\n",
    "    rew: jnp.array  # 報酬行列\n",
    "    P: jnp.array  # 遷移確率行列\n",
    "    mu: jnp.array  # 初期分布\n",
    "    optimal_Q: Optional[jnp.ndarray] = None  # 最適Q値\n",
    "    optimal_V: Optional[jnp.ndarray] = None  # 最適V値\n",
    "\n",
    "    @property\n",
    "    def S(self) -> int:  # 状態空間のサイズ\n",
    "        return len(self.S_set)\n",
    "\n",
    "    @property\n",
    "    def A(self) -> int:  # 行動空間のサイズ\n",
    "        return len(self.A_set)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_greedy_policy(Q: jnp.ndarray):\n",
    "    \"\"\"Q関数の貪欲方策を返します\n",
    "\n",
    "    Args:\n",
    "        Q (jnp.ndarray): (SxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        greedy_policy (jnp.ndarray): (SxA)の行列\n",
    "    \"\"\"\n",
    "    greedy_policy = jnp.zeros_like(Q)\n",
    "    S, A = Q.shape\n",
    "    greedy_policy = greedy_policy.at[jnp.arange(S), Q.argmax(axis=1)].set(1)\n",
    "    assert greedy_policy.shape == (S, A)\n",
    "    return greedy_policy\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"S\", \"A\"))\n",
    "def _compute_optimal_Q(mdp: MDP, S: int, A: int):\n",
    "    \"\"\"MDPについて、ベルマン最適作用素を複数回走らせて最適価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "\n",
    "    Returns:\n",
    "        optimal_Q (jnp.ndarray): (SxA)の行列\n",
    "    \"\"\"\n",
    "\n",
    "    def backup(optimal_Q):\n",
    "        next_v = mdp.P @ optimal_Q.max(axis=1)\n",
    "        assert next_v.shape == (S, A)\n",
    "        return mdp.rew + mdp.gamma * next_v\n",
    "    \n",
    "    optimal_Q = jnp.zeros((S, A))\n",
    "    body_fn = lambda i, Q: backup(Q)\n",
    "    return jax.lax.fori_loop(0, mdp.H + 100, body_fn, optimal_Q)\n",
    "\n",
    "compute_optimal_Q = lambda mdp: _compute_optimal_Q(mdp, mdp.S, mdp.A)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_Q(mdp: MDP, policy: jnp.ndarray):\n",
    "    \"\"\"MDPと方策について、ベルマン期待作用素を複数回走らせて価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        policy (jnp.ndarray): (SxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        optimal_Q (jnp.ndarray): (SxA)の行列\n",
    "    \"\"\"\n",
    "    S, A = policy.shape\n",
    "    chex.assert_shape(policy, (mdp.S, mdp.A))\n",
    "\n",
    "    def backup(policy_Q):\n",
    "        max_Q = (policy * policy_Q).sum(axis=1)\n",
    "        next_v = mdp.P @ max_Q\n",
    "        assert next_v.shape == (S, A)\n",
    "        return mdp.rew + mdp.gamma * next_v\n",
    "    \n",
    "    policy_Q = jnp.zeros((S, A))\n",
    "    body_fn = lambda i, Q: backup(Q)\n",
    "    return jax.lax.fori_loop(0, mdp.H + 100, body_fn, policy_Q)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_matrix(policy: jnp.ndarray):\n",
    "    \"\"\"\n",
    "    上で定義した方策行列を計算します。方策についての内積が取りたいときに便利です。\n",
    "    Args:\n",
    "        policy (jnp.ndarray): (SxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        policy_matrix (jnp.ndarray): (SxSA)の行列\n",
    "    \"\"\"\n",
    "    S, A = policy.shape\n",
    "    PI = policy.reshape(1, S, A)\n",
    "    PI = jnp.tile(PI, (S, 1, 1))\n",
    "    eyes = jnp.eye(S).reshape(S, S, 1)\n",
    "    PI = (eyes * PI).reshape(S, S*A)\n",
    "    return PI\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"N\",))\n",
    "def sample_next_state(mdp: MDP, N: int, key: PRNGKey, D: jnp.array):\n",
    "    \"\"\" 遷移行列Pに従って次の状態をN個サンプルします\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        N (int): サンプルする個数\n",
    "        key (PRNGKey)\n",
    "        D (jnp.ndarray): 状態行動対の集合 [(s1, a1), (s2, a2), ...]\n",
    "\n",
    "    Returns:\n",
    "        new_key (PRNGKey)\n",
    "        next_s_set (jnp.ndarray): (len(D) x N) の次状態の集合\n",
    "        count_SAS (jnp.ndarray): 各(状態, 行動, 次状態)のペアの出現回数を格納した(S x A x S) の行列\n",
    "    \"\"\"\n",
    "\n",
    "    # 次状態をサンプルします\n",
    "    new_key, key = jax.random.split(key)\n",
    "    keys = jax.random.split(key, num=len(D))\n",
    "    @jax.vmap\n",
    "    def choice(key, sa):\n",
    "        return jax.random.choice(key, mdp.S_set, shape=(N,), p=P[sa[0], sa[1]])\n",
    "    next_s = choice(keys, D)\n",
    "\n",
    "    # 集めたサンプルについて、(s, a, ns)が何個出たかカウントします。\n",
    "    S, A, S = mdp.P.shape\n",
    "    count_SAS = jnp.zeros((S*A, S))\n",
    "    count_D_next_S = jax.vmap(lambda next_s: jnp.bincount(next_s, minlength=S, length=S))(next_s)\n",
    "    D_ravel = jnp.ravel_multi_index(D.T, (S, A), mode=\"wrap\")\n",
    "    count_SAS = count_SAS.at[D_ravel].add(count_D_next_S)\n",
    "    return new_key, next_s, count_SAS\n",
    "\n",
    "\n",
    "\n",
    "H = int(1 / (1 - gamma))\n",
    "mdp = MDP(S_set, A_set, gamma, H, rew, P, mu)\n",
    "\n",
    "print(\"状態数：\", mdp.S)\n",
    "print(\"行動数：\", mdp.A)\n",
    "print(\"割引率：\", mdp.gamma)\n",
    "print(\"ホライゾン：\", mdp.H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ロバストVI\n",
    "![](https://cdn.mathpix.com/cropped/2024_09_18_8d84cb03348f28bfa907g-12.jpg?height=603&width=1237&top_left_y=244&top_left_x=317)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Matrices Shape: (10, 3, 10)\n"
     ]
    }
   ],
   "source": [
    "from jax import random\n",
    "V = jnp.zeros((mdp.S))\n",
    "\n",
    "\n",
    "def create_state_dependent_transition_matrices(key,S,A):\n",
    "    \"\"\"\n",
    "    状態数分の遷移関数を作ります。\n",
    "    \n",
    "    Args:\n",
    "        key:乱数生成用パラメータ\n",
    "        S:状態数\n",
    "        A:行動数\n",
    "    \n",
    "    Return:\n",
    "        transition_matrices:遷移関数の配列\n",
    "    \n",
    "    \"\"\"\n",
    "    keys = random.split(key,num=S)\n",
    "\n",
    "    def create_transition_matrix(key):\n",
    "        P = random.uniform(key, shape=(A, S))  # 各状態について乱数を生成\n",
    "        P = P / jnp.sum(P, axis=1, keepdims=True)  # 各状態について正規化\n",
    "        return P\n",
    "    transition_matrices = jax.vmap(create_transition_matrix)(jnp.array(keys))\n",
    "    return transition_matrices\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "transition_matrices = create_state_dependent_transition_matrices(key, mdp.S, mdp.A)\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 非凸二重ループアルゴリズム(RMDPではない)\n",
    "\n",
    "Stochastic Recursive Gradient Descent Ascent for Stochastic Nonconvex-Strongly-Concave Minimax Problems\n",
    "\n",
    "Solving a Class of Non-Convex Min-Max Games Using Iterative First Order Methods\n",
    "\n",
    "(Jin et al., 2020; Luo et al., 2020; Razaviyayn et al., 2020; Zhang et al., 2020）\n",
    "\n",
    "\n",
    "* 内側ループを一定以上の精度で解ければ、rectangluarの仮定がなくても最適方策を学習できます(←まじ？)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方策勾配法 for RMDP\n",
    "* 拡張Mirror discent法Li et al., 2022\n",
    "\t* (s,a)-rectangularを仮定したうえで解かれている。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 情報の断片\n",
    "**遷移、報酬の推定誤差が及ぼす影響**\n",
    "Mannorらは、価値関数が、遷移関数や報酬関数の推定誤差に敏感であるという可能性を示しました。\n",
    "\n",
    "\n",
    "**RMDPは何に対してロバストか**\n",
    "RMDPの解(最適価値関数)は遷移確率や報酬関数の推定誤差に鈍感です。つまり推定誤差に対してロバストです。\n",
    "\n",
    "**rectangularあれこれ**\n",
    "* 低ランクMDP（線形MDP）の場合、$r$-rectangularという仮定を使う。\n",
    "* $s$-rectangularは(s,a)-rectangularより保守的[75]であり、この仮定を用いた研究は、(Le Tallec, 2007; Wiesemann et al., 2013; Derman et al., 2021; Wang et al., 2022)です。\n",
    "\n",
    "**RMDPの課題**\n",
    "* RMDPの総報酬(=価値関数)は方策に関して微分可能ではないし、凸でもないです。つまり劣勾配は存在しない。(劣勾配は、必ずしも微分可能でない凸関数の上で定義されるため)\n",
    "* 価値関数をモロー包絡線という凸性をもった関数で近似することを考えますが、RMDPにおいては最適方策を獲得するために十分であることが示されています。\n",
    "* 近似することで凸になるので劣勾配を求めることができ、射影勾配法が使えるのでは\n",
    "\n",
    "**RMDPについて**\n",
    "$$\n",
    "\n",
    "\\min _{\\boldsymbol{\\pi} \\in \\Pi} \\max _{\\boldsymbol{p} \\in \\mathcal{P}} J_{\\boldsymbol{\\rho}}(\\boldsymbol{\\pi}, \\boldsymbol{p}):=\\boldsymbol{\\rho}^{\\top} \\boldsymbol{v}^{\\boldsymbol{\\pi}, \\boldsymbol{p}}=\\sum_{s \\in \\mathcal{S}} \\rho_{s} v_{s}^{\\boldsymbol{\\pi}, \\boldsymbol{p}} \\tag{2}\n",
    "$$\n",
    "未知の真の遷移カーネルが含まれるようにすることで、(2)の最適方策は実際には信頼性の高いパフォーマンスを実現できます（Russell & Petrik, 2019; Behzadian et al., 2021b; Panaganti et al., 2022）\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
