{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Processes ~Discrete Stochastic Dynamic Programming~\n",
    "\n",
    "このページでは、MARTIN L. PUTERMAN先生が執筆された[Markov Decision Processes](https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887)のメモやまとめを書いていきます。\n",
    "\n",
    "### Infinite-Horizon Model\n",
    "\n",
    "#### 仮定など\n",
    "* 報酬や遷移確率、方策は時間変化しないような定常的な問題を扱います。\n",
    "* 断りのない限りは離散の有限または可算無限の状態空間$S$を仮定します。\n",
    "* 本ページでは収束とは各点収束を示します。つまり各$s \\in S$での個別の極限を考えます。\n",
    "* 級数の極限は、複数の極限を持たないただ一つの極限を持つ場合、存在するとします。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方策\n",
    "\n",
    "Infinite-Horizonの設定では、各方策$\\pi=\\left(d_{1}, d_{2}, \\ldots\\right)$は、以下のような2つの確率変数の確率過程を生成します。\n",
    "$$\\left\\{\\left(X_{t}, r\\left(X_{t}, Y_{t}\\right)\\right) ; t=1,2, \\ldots\\right\\}$$\n",
    "\n",
    "* $X_{t}$：ある時刻$t$での状態を示しています。これは遷移確率や確率的方策などに依存するため、確率変数です。\n",
    "* r\\left(X_{t}, Y_{t}\\right)：$X_{t}で行動Y_{t}を行った場合に得られる報酬関数を示します。これも確率変数です。\n",
    "\n",
    "##### 決定論的方策\n",
    "* マルコフ方策の場合：$Y_{t}=d_{t}\\left(X_{t}\\right) \\quad \\text { for } d_{t} \\in D^{\\mathrm{MD}} \\quad $\n",
    "* 履歴依存方策の場合：$Y_{t}=d_{t}\\left(Z_{t}\\right) \\quad \\text { for } d_{t} \\in D^{\\mathrm{HD}}$\n",
    "  * $Z_{t}$は、時刻$t$までの履歴であり、確率変数です。\n",
    "\n",
    "##### 確率論的方策\n",
    "* マルコフ方策の場合：$P\\left\\{Y_{t}=a\\right\\}=q_{d_{t}\\left(x_{t}\\right)}(a) \\quad \\text { for } d_{t} \\in D^{\\mathrm{MR}}$\n",
    "* 履歴依存方策の場合：$P\\left\\{Y_{t}=a\\right\\}=q_{d_{t}\\left(z_{t}\\right)}(a) \\quad \\text { for } d_{t} \\in D^{\\mathrm{HR}}$\n",
    "  * $t$において$a$をとる確率は、方策$d_t$で条件付けされた確率測度$q$が$a$をとる確率に等しいです。\n",
    "\n",
    "##### 方策$\\pi \\in \\Pi^{\\mathrm{HR}}$の期待累積報酬(無限MDP)\n",
    "$$\n",
    "\\begin{equation*}\n",
    "v^{\\pi}(s) \\equiv \\lim _{N \\rightarrow \\infty} E_{s}^{\\pi}\\left\\{\\sum_{t=1}^{N} r\\left(X_{t}, Y_{t}\\right)\\right\\}=\\lim _{N \\rightarrow \\infty} v_{N+1}^{\\pi}(s) \\tag{5.1.1}\n",
    "\\end{equation*}\n",
    "$$\n",
    "* $v_{N+1}^{\\pi}(s)$は$N$回方策が行動を行い、終端状態における報酬が$0$のMDPという仮定の下での期待累積報酬を表しています。この仮定は、**TODO**\n",
    "* 上式は報酬がゼロ以外の場合かつ状態、行動が1つしかないような場合に$+\\infty$または$-\\infty$に発散する場合があります。\n",
    "\n",
    "##### 方策$\\pi \\in \\Pi^{\\mathrm{HR}}$の*割引*累積報酬(割引無限MDP)\n",
    "$0 \\leq \\lambda<1$に対して、\n",
    "$$\n",
    "\\begin{equation*}\n",
    "v_{\\lambda}^{\\pi}(s) \\equiv \\lim _{N \\rightarrow \\infty} E_{s}^{\\pi}\\left\\{\\sum_{t=1}^{N} \\lambda^{t-1} r\\left(X_{t}, Y_{t}\\right)\\right\\} \\tag{5.1.3}\n",
    "\\end{equation*}\n",
    "$$\n",
    "* 5.1.3の極限は、$r$を無限大より小さい値$M$で上から抑えられる場合に存在します。つまり、$\\sup _{s \\in S} \\sup _{a \\in A_{s}}|r(s, a)|=M<\\infty$の場合です。\n",
    "\n",
    "\n",
    "##### 方策$\\pi \\in \\Pi^{\\mathrm{HR}}$の平均報酬(ゲイン)\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "g^{\\pi}(s) \\equiv \\lim _{N \\rightarrow \\infty} \\frac{1}{N} E_{s}^{\\pi}\\left\\{\\sum_{t=1}^{N} r\\left(X_{t}, Y_{t}\\right)\\right\\}=\\lim _{N \\rightarrow \\infty} \\frac{1}{N} v_{N+1}^{\\pi}(s) \\tag{5.1.6}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "* 5.1.6が極限を持たない場合、平均報酬の極限の上界と下界をそれぞれlim sup平均報酬$g_{+}$、lim inf平均報酬$g_{-}$として以下のように定義します。\n",
    "$$\n",
    "g_{-}^{\\pi}(s) \\equiv \\liminf _{N \\rightarrow \\infty} \\frac{1}{N} v_{N+1}^{\\pi}(s), \\quad g_{+}^{\\pi}(s) \\equiv \\limsup _{N \\rightarrow \\infty} \\frac{1}{N} v_{N+1}^{\\pi}(s)\n",
    "$$\n",
    "* これは、方策\\piであるエピソードにおいて達成可能な平均報酬の上界と下界を示しています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
